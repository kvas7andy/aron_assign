{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter posts\n",
    "*Marcin Zabłocki*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Abstract\n",
    "The goal of this project was to predict sentiment for the given Twitter post using Python. Sentiment analysis can predict many different emotions attached to the text, but in this report only 3 major were considered: positive, negative and neutral. The training dataset was small (just over 5900 examples) and the data within it was highly skewed, which greatly impacted on the difficulty of building good classifier. After creating a lot of custom features, utilizing both bag-of-words and word2vec representations and applying the Extreme Gradient Boosting algorithm, the classification accuracy at level of 58% was achieved.\n",
    "\n",
    "\n",
    "## Used Python Libraries\n",
    "Data was pre-processed using *pandas*, *gensim* and *numpy* libraries and the learning/validating process was built with *scikit-learn*. Plots were created using *plotly*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "import gensim\n",
    "from os import path\n",
    "\n",
    "# plotly configuration\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "dataset_dir = '~/Documents/aaron_assign'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook code convention\n",
    "This report was first prepared as a classical Python project using object oriented programming with maintainability in mind. In order to show this project as a Jupyter Notebook, the classes had to be splitted into multiple code-cells. In order to do so, the classes are suffixed with *_PurposeOfThisSnippet* name and they inherit one from another. The final class will be then run and the results will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "The input data consisted two CSV files:\n",
    "`train.csv` (5971 tweets) and `test.csv` (4000 tweets) - one for training and one for testing.\n",
    "Format of the data was the following (test data didn't contain Category column):\n",
    "\n",
    "\n",
    "| Id | Category  | Tweet |\n",
    "|------|------|------|\n",
    "|   635930169241374720  | neutral | IOS 9 App Transport Security. Mm need to check if my 3rd party network pod supports it |\n",
    "\n",
    "All tweets are in english, so it simplifies the processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "## Loading the data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        else:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"text\"],dtype={\"id\":\"int64\",\"text\":\"str\"},nrows=4000)\n",
    "            not_null_text = 1 ^ pd.isnull(self.data[\"text\"])\n",
    "            not_null_id = 1 ^ pd.isnull(self.data[\"id\"])\n",
    "            self.data = self.data.loc[not_null_id & not_null_text, :]\n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take 1.6 million tweets from Sentiment 140 dataset, filter out and make training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains the following 6 fields:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "ids: The id of the tweet ( 2087)\n",
    "\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path.join(dataset_dir, 'training.1600000.processed.noemoticon.csv'), encoding = \"ISO-8859-1\")\n",
    "data.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "data = data[[\"ids\", \"target\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>negative</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>negative</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>negative</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1467811592</td>\n",
       "      <td>negative</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467811594</td>\n",
       "      <td>negative</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1467811795</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1467812025</td>\n",
       "      <td>negative</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1467812416</td>\n",
       "      <td>negative</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1467812579</td>\n",
       "      <td>negative</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1467812723</td>\n",
       "      <td>negative</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1467812771</td>\n",
       "      <td>negative</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1467812784</td>\n",
       "      <td>negative</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1467812799</td>\n",
       "      <td>negative</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1467812964</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1467813137</td>\n",
       "      <td>negative</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1467813579</td>\n",
       "      <td>negative</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1467813782</td>\n",
       "      <td>negative</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1467813985</td>\n",
       "      <td>negative</td>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1467813992</td>\n",
       "      <td>negative</td>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1467814119</td>\n",
       "      <td>negative</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1467814180</td>\n",
       "      <td>negative</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1467814192</td>\n",
       "      <td>negative</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1467814438</td>\n",
       "      <td>negative</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1467814783</td>\n",
       "      <td>negative</td>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1467814883</td>\n",
       "      <td>negative</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1467815199</td>\n",
       "      <td>negative</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1467815753</td>\n",
       "      <td>negative</td>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1467815923</td>\n",
       "      <td>negative</td>\n",
       "      <td>some1 hacked my account on aim  now i have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599969</th>\n",
       "      <td>2193578196</td>\n",
       "      <td>positive</td>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>2193578237</td>\n",
       "      <td>positive</td>\n",
       "      <td>@marttn thanks Martin. not the most imaginativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>2193578269</td>\n",
       "      <td>positive</td>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>2193578319</td>\n",
       "      <td>positive</td>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>2193578345</td>\n",
       "      <td>positive</td>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>2193578347</td>\n",
       "      <td>positive</td>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>2193578348</td>\n",
       "      <td>positive</td>\n",
       "      <td>@mattycus Yeah, my conscience would be clear i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>2193578386</td>\n",
       "      <td>positive</td>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>2193578395</td>\n",
       "      <td>positive</td>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>2193578576</td>\n",
       "      <td>positive</td>\n",
       "      <td>In the garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>2193578679</td>\n",
       "      <td>positive</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>2193578716</td>\n",
       "      <td>positive</td>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>2193578739</td>\n",
       "      <td>positive</td>\n",
       "      <td>@thrillmesoon i figured out how to see my twee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>2193578758</td>\n",
       "      <td>positive</td>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>2193578847</td>\n",
       "      <td>positive</td>\n",
       "      <td>You heard it here first -- We're having a girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>2193578982</td>\n",
       "      <td>positive</td>\n",
       "      <td>if ur the lead singer in a band, beware fallin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>2193579087</td>\n",
       "      <td>positive</td>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>2193579092</td>\n",
       "      <td>positive</td>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>2193579191</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>2193579211</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>2193579249</td>\n",
       "      <td>positive</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>2193579284</td>\n",
       "      <td>positive</td>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>2193579434</td>\n",
       "      <td>positive</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>2193579477</td>\n",
       "      <td>positive</td>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>2193579489</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>2193601966</td>\n",
       "      <td>positive</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>2193601969</td>\n",
       "      <td>positive</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>2193601991</td>\n",
       "      <td>positive</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>2193602064</td>\n",
       "      <td>positive</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>2193602129</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   emotion  \\\n",
       "0        1467810672  negative   \n",
       "1        1467810917  negative   \n",
       "2        1467811184  negative   \n",
       "3        1467811193  negative   \n",
       "4        1467811372  negative   \n",
       "5        1467811592  negative   \n",
       "6        1467811594  negative   \n",
       "7        1467811795  negative   \n",
       "8        1467812025  negative   \n",
       "9        1467812416  negative   \n",
       "10       1467812579  negative   \n",
       "11       1467812723  negative   \n",
       "12       1467812771  negative   \n",
       "13       1467812784  negative   \n",
       "14       1467812799  negative   \n",
       "15       1467812964  negative   \n",
       "16       1467813137  negative   \n",
       "17       1467813579  negative   \n",
       "18       1467813782  negative   \n",
       "19       1467813985  negative   \n",
       "20       1467813992  negative   \n",
       "21       1467814119  negative   \n",
       "22       1467814180  negative   \n",
       "23       1467814192  negative   \n",
       "24       1467814438  negative   \n",
       "25       1467814783  negative   \n",
       "26       1467814883  negative   \n",
       "27       1467815199  negative   \n",
       "28       1467815753  negative   \n",
       "29       1467815923  negative   \n",
       "...             ...       ...   \n",
       "1599969  2193578196  positive   \n",
       "1599970  2193578237  positive   \n",
       "1599971  2193578269  positive   \n",
       "1599972  2193578319  positive   \n",
       "1599973  2193578345  positive   \n",
       "1599974  2193578347  positive   \n",
       "1599975  2193578348  positive   \n",
       "1599976  2193578386  positive   \n",
       "1599977  2193578395  positive   \n",
       "1599978  2193578576  positive   \n",
       "1599979  2193578679  positive   \n",
       "1599980  2193578716  positive   \n",
       "1599981  2193578739  positive   \n",
       "1599982  2193578758  positive   \n",
       "1599983  2193578847  positive   \n",
       "1599984  2193578982  positive   \n",
       "1599985  2193579087  positive   \n",
       "1599986  2193579092  positive   \n",
       "1599987  2193579191  positive   \n",
       "1599988  2193579211  positive   \n",
       "1599989  2193579249  positive   \n",
       "1599990  2193579284  positive   \n",
       "1599991  2193579434  positive   \n",
       "1599992  2193579477  positive   \n",
       "1599993  2193579489  positive   \n",
       "1599994  2193601966  positive   \n",
       "1599995  2193601969  positive   \n",
       "1599996  2193601991  positive   \n",
       "1599997  2193602064  positive   \n",
       "1599998  2193602129  positive   \n",
       "\n",
       "                                                      text  \n",
       "0        is upset that he can't update his Facebook by ...  \n",
       "1        @Kenichan I dived many times for the ball. Man...  \n",
       "2          my whole body feels itchy and like its on fire   \n",
       "3        @nationwideclass no, it's not behaving at all....  \n",
       "4                            @Kwesidei not the whole crew   \n",
       "5                                              Need a hug   \n",
       "6        @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "7                     @Tatiana_K nope they didn't have it   \n",
       "8                                @twittera que me muera ?   \n",
       "9              spring break in plain city... it's snowing   \n",
       "10                              I just re-pierced my ears   \n",
       "11       @caregiving I couldn't bear to watch it.  And ...  \n",
       "12       @octolinz16 It it counts, idk why I did either...  \n",
       "13       @smarrison i would've been the first, but i di...  \n",
       "14       @iamjazzyfizzle I wish I got to watch it with ...  \n",
       "15       Hollis' death scene will hurt me severely to w...  \n",
       "16                                    about to file taxes   \n",
       "17       @LettyA ahh ive always wanted to see rent  lov...  \n",
       "18       @FakerPattyPattz Oh dear. Were you drinking ou...  \n",
       "19       @alydesigns i was out most of the day so didn'...  \n",
       "20       one of my friend called me, and asked to meet ...  \n",
       "21        @angry_barista I baked you a cake but I ated it   \n",
       "22                  this week is not going as i had hoped   \n",
       "23                              blagh class at 8 tomorrow   \n",
       "24          I hate when I have to call and wake people up   \n",
       "25       Just going to cry myself to sleep after watchi...  \n",
       "26                                  im sad now  Miss.Lilly  \n",
       "27       ooooh.... LOL  that leslie.... and ok I won't ...  \n",
       "28       Meh... Almost Lover is the exception... this t...  \n",
       "29       some1 hacked my account on aim  now i have to ...  \n",
       "...                                                    ...  \n",
       "1599969  Thanks @eastwestchic &amp; @wangyip Thanks! Th...  \n",
       "1599970  @marttn thanks Martin. not the most imaginativ...  \n",
       "1599971          @MikeJonesPhoto Congrats Mike  Way to go!  \n",
       "1599972  http://twitpic.com/7jp4n - OMG! Office Space.....  \n",
       "1599973  @yrclndstnlvr ahaha nooo you were just away fr...  \n",
       "1599974  @BizCoachDeb  Hey, I'm baack! And, thanks so m...  \n",
       "1599975  @mattycus Yeah, my conscience would be clear i...  \n",
       "1599976  @MayorDorisWolfe Thats my girl - dishing out t...  \n",
       "1599977                          @shebbs123 i second that   \n",
       "1599978                                     In the garden   \n",
       "1599979  @myheartandmind jo jen by nemuselo zrovna tÃ© ...  \n",
       "1599980  Another Commenting Contest! [;: Yay!!!  http:/...  \n",
       "1599981  @thrillmesoon i figured out how to see my twee...  \n",
       "1599982  @oxhot theri tomorrow, drinking coffee, talkin...  \n",
       "1599983  You heard it here first -- We're having a girl...  \n",
       "1599984  if ur the lead singer in a band, beware fallin...  \n",
       "1599985              @tarayqueen too much ads on my blog.   \n",
       "1599986  @La_r_a NEVEER  I think that you both will get...  \n",
       "1599987  @Roy_Everitt ha- good job. that's right - we g...  \n",
       "1599988                 @Ms_Hip_Hop im glad ur doing well   \n",
       "1599989                              WOOOOO! Xbox is back   \n",
       "1599990  @rmedina @LaTati Mmmm  That sounds absolutely ...  \n",
       "1599991                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd   \n",
       "1599992                                  @SCOOBY_GRITBOYS   \n",
       "1599993  @Cliff_Forster Yeah, that does work better tha...  \n",
       "1599994  Just woke up. Having no school is the best fee...  \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1599999 rows x 3 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"id\", \"emotion\", \"text\"]\n",
    "def int_to_label(x):\n",
    "    if x == 0:\n",
    "        return 'negative'\n",
    "    if x == 2:\n",
    "        return 'neutral'\n",
    "    if x == 4:\n",
    "        return 'positive'\n",
    "    \n",
    "data['emotion'] = data[['emotion']].applymap(int_to_label)\n",
    "data[::16].to_csv('data/train16.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet above is prepared, to load the data form the given file for further processing, or just read already preprocessed file from the cache.\n",
    "There's also a distinction between processing testing and training data. As the ```test.csv``` file was full of empty entries, they were removed.\n",
    "Additional class properties such as data_model, wordlist etc. will be used further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467813137</td>\n",
       "      <td>negative</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467816149</td>\n",
       "      <td>negative</td>\n",
       "      <td>@julieebaby awe i love you too!!!! 1 am here  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467820835</td>\n",
       "      <td>negative</td>\n",
       "      <td>@tea oh! i'm so sorry  i didn't think about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467824199</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Starrbby too bad I won't be around I lost my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467834227</td>\n",
       "      <td>negative</td>\n",
       "      <td>@statravelAU just got ur newsletter, those far...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   emotion                                               text\n",
       "0  1467813137  negative                               about to file taxes \n",
       "1  1467816149  negative  @julieebaby awe i love you too!!!! 1 am here  ...\n",
       "2  1467820835  negative  @tea oh! i'm so sorry  i didn't think about th...\n",
       "3  1467824199  negative  @Starrbby too bad I won't be around I lost my ...\n",
       "4  1467834227  negative  @statravelAU just got ur newsletter, those far..."
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"data/train16.csv\")\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution\n",
    "First thing that can be done as soon as the data is loaded is to see the data distribution. The training set had the following distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "08c50f3e-300c-4dc9-894b-560979ec6fa0",
         "x": [
          "negative",
          "neutral",
          "positive"
         ],
         "y": [
          49999,
          0,
          50000
         ]
        }
       ],
       "layout": {
        "title": "Sentiment type distribution in training set"
       }
      },
      "text/html": [
       "<div id=\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\", [{\"x\": [\"negative\", \"neutral\", \"positive\"], \"y\": [49999, 0, 50000], \"type\": \"bar\", \"uid\": \"d1dda875-2546-4f03-878f-19a7bec7c779\"}], {\"title\": \"Sentiment type distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\", [{\"x\": [\"negative\", \"neutral\", \"positive\"], \"y\": [49999, 0, 50000], \"type\": \"bar\", \"uid\": \"d1dda875-2546-4f03-878f-19a7bec7c779\"}], {\"title\": \"Sentiment type distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"8909d9f8-8b84-4c0a-9d5d-353fa747ff98\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data.processed_data\n",
    "neg = len(df[df[\"emotion\"] == \"negative\"])\n",
    "pos = len(df[df[\"emotion\"] == \"positive\"])\n",
    "neu = len(df[df[\"emotion\"] == \"neutral\"])\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=[\"negative\",\"neutral\",\"positive\"],\n",
    "        y=[neg, neu, pos],\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Sentiment type distribution in training set\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "The targed of the following preprocessing is to create a **Bag-of-Words** representation of the data. The steps will execute as follows:\n",
    "1. Cleansing\n",
    "<ol style=\"list-style-type:decimal\"><li>Remove URLs</li>\n",
    "<li>Remove usernames (mentions)</li>\n",
    "<li>Remove tweets with *Not Available* text</li>\n",
    "<li>Remove special characters</li>\n",
    "<li>Remove numbers</li></ol>\n",
    "1. Text processing\n",
    "<ol style=\"list-style-type:decimal\">\n",
    "<li>Tokenize</li>\n",
    "<li>Transform to lowercase</li>\n",
    "<li>Stem</li></ol>\n",
    "1. Build word list for Bag-of-Words\n",
    "\n",
    "### Cleansing\n",
    "For the purpose of cleansing, the ```TwitterCleanup``` class was created. It consists methods allowing to execute all of the tasks show in the list above. Most of those is done using regular expressions.\n",
    "The class exposes it's interface through ```iterate()``` method - it yields every cleanup method in proper order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterCleanuper:\n",
    "    def iterate(self):\n",
    "        for cleanup_method in [self.remove_urls,\n",
    "                               self.remove_usernames,\n",
    "                               self.remove_na,\n",
    "                               self.remove_special_chars,\n",
    "                               self.remove_numbers]:\n",
    "            yield cleanup_method\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_by_regex(tweets, regexp):\n",
    "        tweets.loc[:, \"text\"].replace(regexp, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_urls(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_na(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"Not Available\"]\n",
    "\n",
    "    def remove_special_chars(self, tweets):  # it unrolls the hashtags to normal words\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_usernames(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_numbers(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded tweets can be now cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Cleansing(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    def cleanup(self, cleanuper):\n",
    "        t = self.processed_data\n",
    "        for cleanup_method in cleanuper.iterate():\n",
    "            if not self.is_testing:\n",
    "                t = cleanup_method(t)\n",
    "            else:\n",
    "                if cleanup_method.__name__ != \"remove_na\":\n",
    "                    t = cleanup_method(t)\n",
    "\n",
    "        self.processed_data = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467813137</td>\n",
       "      <td>negative</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467816149</td>\n",
       "      <td>negative</td>\n",
       "      <td>awe i love you too am here  i miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467820835</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh im so sorry  i didnt think about that befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467824199</td>\n",
       "      <td>negative</td>\n",
       "      <td>too bad I wont be around I lost my job and can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467834227</td>\n",
       "      <td>negative</td>\n",
       "      <td>just got ur newsletter those fares really are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   emotion                                               text\n",
       "0  1467813137  negative                               about to file taxes \n",
       "1  1467816149  negative             awe i love you too am here  i miss you\n",
       "2  1467820835  negative  oh im so sorry  i didnt think about that befor...\n",
       "3  1467824199  negative  too bad I wont be around I lost my job and can...\n",
       "4  1467834227  negative  just got ur newsletter those fares really are ..."
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Cleansing(data)\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & stemming\n",
    "For the text processing, ```nltk``` library is used. First, the tweets are tokenized using ```nlkt.word_tokenize``` and then, stemming is done using **PorterStemmer** as the tweets are 100% in english.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_TokenStem(TwitterData_Cleansing):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    def stem(self, stemmer=nltk.PorterStemmer()):\n",
    "        def stem_and_join(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "\n",
    "    def tokenize(self, tokenizer=nltk.word_tokenize):data = TwitterData_Initialize()\n",
    "data.initialize(\"data/train16.csv\")\n",
    "data = TwitterData_Cleansing(data)\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data = TwitterData_Wordlist(data)\n",
    "data.build_wordlist()\n",
    "        def tokenize_row(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenized_text\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(tokenize_row, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467813137</td>\n",
       "      <td>negative</td>\n",
       "      <td>[about, to, file, tax]</td>\n",
       "      <td>[about, to, file, taxes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467816149</td>\n",
       "      <td>negative</td>\n",
       "      <td>[awe, i, love, you, too, am, here, i, miss, you]</td>\n",
       "      <td>[awe, i, love, you, too, am, here, i, miss, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467820835</td>\n",
       "      <td>negative</td>\n",
       "      <td>[oh, im, so, sorri, i, didnt, think, about, th...</td>\n",
       "      <td>[oh, im, so, sorry, i, didnt, think, about, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467824199</td>\n",
       "      <td>negative</td>\n",
       "      <td>[too, bad, i, wont, be, around, i, lost, my, j...</td>\n",
       "      <td>[too, bad, I, wont, be, around, I, lost, my, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467834227</td>\n",
       "      <td>negative</td>\n",
       "      <td>[just, got, ur, newslett, those, fare, realli,...</td>\n",
       "      <td>[just, got, ur, newsletter, those, fares, real...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   emotion                                               text  \\\n",
       "0  1467813137  negative                             [about, to, file, tax]   \n",
       "1  1467816149  negative   [awe, i, love, you, too, am, here, i, miss, you]   \n",
       "2  1467820835  negative  [oh, im, so, sorri, i, didnt, think, about, th...   \n",
       "3  1467824199  negative  [too, bad, i, wont, be, around, i, lost, my, j...   \n",
       "4  1467834227  negative  [just, got, ur, newslett, those, fare, realli,...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0                           [about, to, file, taxes]  \n",
       "1   [awe, i, love, you, too, am, here, i, miss, you]  \n",
       "2  [oh, im, so, sorry, i, didnt, think, about, th...  \n",
       "3  [too, bad, I, wont, be, around, I, lost, my, j...  \n",
       "4  [just, got, ur, newsletter, those, fares, real...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_TokenStem(data)\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the wordlist\n",
    "The wordlist (dictionary) is build by simple count of occurences of every unique word across all of the training dataset.\n",
    "\n",
    "Before building the final wordlist for the model, let's take a look at the non-filtered version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 47319), ('to', 35180), ('the', 32192), ('a', 23709), ('it', 20965)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = Counter()\n",
    "for idx in data.processed_data.index:\n",
    "    words.update(data.processed_data.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commont words (as expected) are the typical english stopwords. We will filter them out, however, as purpose of this analysis is to determine sentiment, words like \"not\" and \"n't\" can influence it greatly. Having this in mind, this word will be whitelisted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('im', 11097), ('go', 8598), ('get', 6750), ('not', 6594), ('wa', 6560)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, there are some words that seem too be occuring to many times, let's filter them. After some analysis, the lower bound was set to 3.\n",
    "\n",
    "The wordlist is also saved to the csv file, so the same words can be used for the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[\"thank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_word_gen = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:data_before_word_gen is <__main__.TwitterData_TokenStem object at 0x7f0fc2610c18>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%store data_before_word_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable data_before_word_gen\n"
     ]
    }
   ],
   "source": [
    "%store -r data_before_word_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_before_word_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Wordlist(TwitterData_TokenStem):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    whitelist = [\"n't\",\"not\"]\n",
    "    wordlist = []\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=100, max_occurences=np.inf, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile(\"data/wordlist.csv\"):\n",
    "           word_df = pd.read_csv(\"data/wordlist.csv\")\n",
    "           word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "           self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "           return\n",
    "\n",
    "        words = Counter()\n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"text\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"data/wordlist.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TwitterData_Wordlist(data)\n",
    "data.build_wordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'thank' in data.wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "uid": "a5193068-4e26-4d8c-82ce-6a004249665f",
         "x": [
          5107,
          5168,
          5342,
          5670,
          5803,
          6233,
          6560,
          6594,
          6750,
          8598,
          11097
         ],
         "y": [
          "like",
          "love",
          "work",
          "good",
          "thi",
          "day",
          "wa",
          "not",
          "get",
          "go",
          "im"
         ]
        }
       ],
       "layout": {
        "title": "Top words in built wordlist"
       }
      },
      "text/html": [
       "<div id=\"55618c24-3857-4ce1-81fb-de8afcaa4972\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"55618c24-3857-4ce1-81fb-de8afcaa4972\", [{\"orientation\": \"h\", \"x\": [5107, 5168, 5342, 5670, 5803, 6233, 6560, 6594, 6750, 8598, 11097], \"y\": [\"like\", \"love\", \"work\", \"good\", \"thi\", \"day\", \"wa\", \"not\", \"get\", \"go\", \"im\"], \"type\": \"bar\", \"uid\": \"6439d43a-bf66-427d-99a2-f1d068eb2336\"}], {\"title\": \"Top words in built wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"55618c24-3857-4ce1-81fb-de8afcaa4972\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"55618c24-3857-4ce1-81fb-de8afcaa4972\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"55618c24-3857-4ce1-81fb-de8afcaa4972\", [{\"orientation\": \"h\", \"x\": [5107, 5168, 5342, 5670, 5803, 6233, 6560, 6594, 6750, 8598, 11097], \"y\": [\"like\", \"love\", \"work\", \"good\", \"thi\", \"day\", \"wa\", \"not\", \"get\", \"go\", \"im\"], \"type\": \"bar\", \"uid\": \"6439d43a-bf66-427d-99a2-f1d068eb2336\"}], {\"title\": \"Top words in built wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"55618c24-3857-4ce1-81fb-de8afcaa4972\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = pd.read_csv(\"data/wordlist.csv\")\n",
    "x_words = list(words.loc[0:10,\"word\"])\n",
    "x_words.reverse()\n",
    "y_occ = list(words.loc[0:10,\"occurrences\"])\n",
    "y_occ.reverse()\n",
    "\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=y_occ,\n",
    "        y=x_words,\n",
    "        orientation=\"h\"\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Top words in built wordlist\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "The data is ready to transform it to bag-of-words representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_BagOfWords(TwitterData_Wordlist):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"emotion\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data and see, which words are the most common for particular sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TwitterData_BagOfWords(data)\n",
    "\n",
    "bow, labels = data.build_data_model()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['negative', 'positive'], dtype='object', name='label')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "grouped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "name": "positive",
         "type": "bar",
         "uid": "418f5566-95f1-445a-8c8a-167026f7d6e0",
         "x": [
          "im",
          "good",
          "love",
          "go",
          "day",
          "thank",
          "get",
          "not",
          "work",
          "wa",
          "thi"
         ],
         "y": [
          4380,
          3701,
          3615,
          3413,
          3043,
          3029,
          2843,
          1995,
          1649,
          2649,
          2484
         ]
        },
        {
         "name": "negative",
         "type": "bar",
         "uid": "aca1794c-f09c-49bd-9be9-f1d4aea1fcb5",
         "x": [
          "im",
          "good",
          "love",
          "go",
          "day",
          "thank",
          "get",
          "not",
          "work",
          "wa",
          "thi"
         ],
         "y": [
          5906,
          1765,
          1270,
          4650,
          2859,
          493,
          3641,
          4335,
          3446,
          3348,
          3129
         ]
        }
       ],
       "layout": {
        "title": "Most common words across sentiments"
       }
      },
      "text/html": [
       "<div id=\"e461b8c2-c10d-4c49-91fb-a900b7fde495\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e461b8c2-c10d-4c49-91fb-a900b7fde495\", [{\"name\": \"positive\", \"x\": [\"im\", \"good\", \"love\", \"go\", \"day\", \"thank\", \"get\", \"not\", \"work\", \"wa\", \"thi\"], \"y\": [4380, 3701, 3615, 3413, 3043, 3029, 2843, 1995, 1649, 2649, 2484], \"type\": \"bar\", \"uid\": \"8d9e14d7-83d4-4d12-b336-7aea9c3f255a\"}, {\"name\": \"negative\", \"x\": [\"im\", \"good\", \"love\", \"go\", \"day\", \"thank\", \"get\", \"not\", \"work\", \"wa\", \"thi\"], \"y\": [5906, 1765, 1270, 4650, 2859, 493, 3641, 4335, 3446, 3348, 3129], \"type\": \"bar\", \"uid\": \"7dc43f2f-a229-4b13-a9a8-2f01f7cf203e\"}], {\"title\": \"Most common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"e461b8c2-c10d-4c49-91fb-a900b7fde495\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e461b8c2-c10d-4c49-91fb-a900b7fde495\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e461b8c2-c10d-4c49-91fb-a900b7fde495\", [{\"name\": \"positive\", \"x\": [\"im\", \"good\", \"love\", \"go\", \"day\", \"thank\", \"get\", \"not\", \"work\", \"wa\", \"thi\"], \"y\": [4380, 3701, 3615, 3413, 3043, 3029, 2843, 1995, 1649, 2649, 2484], \"type\": \"bar\", \"uid\": \"8d9e14d7-83d4-4d12-b336-7aea9c3f255a\"}, {\"name\": \"negative\", \"x\": [\"im\", \"good\", \"love\", \"go\", \"day\", \"thank\", \"get\", \"not\", \"work\", \"wa\", \"thi\"], \"y\": [5906, 1765, 1270, 4650, 2859, 493, 3641, 4335, 3446, 3348, 3129], \"type\": \"bar\", \"uid\": \"7dc43f2f-a229-4b13-a9a8-2f01f7cf203e\"}], {\"title\": \"Most common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"e461b8c2-c10d-4c49-91fb-a900b7fde495\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "words_to_visualize = []\n",
    "sentiments = [\"positive\",\"negative\",\"neutral\"]\n",
    "#get the most 7 common words for every sentiment\n",
    "for sentiment in sentiments:\n",
    "    if sentiment not in grouped.index:\n",
    "        continue\n",
    "    words = grouped.loc[sentiment,:]\n",
    "    words.sort_values(inplace=True,ascending=False)\n",
    "    for w in words.index[:7]:\n",
    "        if w not in words_to_visualize:\n",
    "            words_to_visualize.append(w)\n",
    "            \n",
    "            \n",
    "#visualize it\n",
    "plot_data = []\n",
    "for sentiment in sentiments:\n",
    "    if sentiment not in grouped.index:\n",
    "        continue\n",
    "    plot_data.append(graph_objs.Bar(\n",
    "            x = [w.split(\"_\")[0] for w in words_to_visualize],\n",
    "            y = [grouped.loc[sentiment,w] for w in words_to_visualize],\n",
    "            name = sentiment\n",
    "    ))\n",
    "\n",
    "plotly.offline.iplot({\n",
    "        \"data\":plot_data,\n",
    "        \"layout\":graph_objs.Layout(title=\"Most common words across sentiments\")\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most common words show high distinction between classes like *go* and *see* and other are occuring in similiar amount for every class (*plan*, *obama*).\n",
    "\n",
    "None of the most common words is unique to the negative class. At this point, it's clear that skewed data distribution will be a problem in distinguishing negative tweets from the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "First of all, lets establish seed for random numbers generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 666\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following utility function will train the classifier and show the F1, precision, recall and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    log(\"\")\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    log(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    log(\"Learing time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    log(\"Predicting time {0}s\".format(time() - now))\n",
    "\n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    log(\"=================== Results ===================\")\n",
    "    log(\"            Negative     Neutral     Positive\")\n",
    "    log(\"F1       \" + str(f1))\n",
    "    log(\"Precision\" + str(precision))\n",
    "    log(\"Recall   \" + str(recall))\n",
    "    log(\"Accuracy \" + str(accuracy))\n",
    "    log(\"===============================================\")\n",
    "\n",
    "    return precision, recall, accuracy, f1\n",
    "\n",
    "def log(x):\n",
    "    #can be used to write to log file\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: BOW + Naive Bayes\n",
    "It is nice to see what kind of results we might get from such simple model. The bag-of-words representation is binary, so Naive Bayes Classifier seems like a nice algorithm to start the experiments.\n",
    "\n",
    "The experiment will be based on ```7:3``` train:test stratified split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Testing BernoulliNB\n",
      "Learing time 2.4626076221466064s\n",
      "Predicting time 0.9015510082244873s\n",
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.73891254 0.75359596]\n",
      "Precision[0.76160487 0.73298462]\n",
      "Recall   [0.71753333 0.7754    ]\n",
      "Accuracy 0.7464666666666666\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=bow.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result with accuracy at level of 58% seems to be quite nice result for such basic algorithm like Naive Bayes (having in mind that random classifier would yield result of around 33% accuracy). This performance may not hold for the final testing set. In order to see how the NaiveBayes performs in more general cases, 8-fold crossvalidation is used. The 8 fold is used, to optimize speed of testing on my 8-core machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(classifier, X_train, y_train):\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    log(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    accuracy = [cross_val_score(classifier, X_train, y_train, cv=8, n_jobs=-1)]\n",
    "    log(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    log(\"Accuracy: \" + str(accuracy[0]))\n",
    "    log(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))\n",
    "    log(\"===============================================\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating BernoulliNB...\n",
      "Crosvalidation completed in 14.390513896942139s\n",
      "Accuracy: [0.74024    0.7324     0.74488    0.75272    0.74304    0.74392\n",
      " 0.74432    0.74461957]\n",
      "Average accuracy: 0.7432674461956956\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "nb_acc = cv(BernoulliNB(), bow.iloc[:,1:], bow.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result no longer looks optimistic. For some of the splits, Naive Bayes classifier showed performance below the performance of random classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on our 100 messages dataset for \"gene editing topic\" reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=bow.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "model = classifier.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TwitterData_BagOfWords(data)\n",
    "\n",
    "bow, labels = data.build_data_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TwitterData()\n",
    "data.initialize(path.join(dataset_dir, \"filtered_gene_testdataset.csv\"))\n",
    "#data.build_features()\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.build_wordlist()\n",
    "data.build_data_model()\n",
    "test_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.66666667 0.83333333]\n",
      "Precision[0.75862069 0.78571429]\n",
      "Recall   [0.59459459 0.88709677]\n",
      "Accuracy 0.7777777777777778\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "list_of_labels = sorted(list(set(y_train)))\n",
    "\n",
    "X_test = test_data.data_model.iloc[:,1:]\n",
    "y_test = test_data.data_model.iloc[:,0]\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "log(\"=================== Results ===================\")\n",
    "log(\"            Negative     Neutral     Positive\")\n",
    "log(\"F1       \" + str(f1))\n",
    "log(\"Precision\" + str(precision))\n",
    "log(\"Recall   \" + str(recall))\n",
    "log(\"Accuracy \" + str(accuracy))\n",
    "log(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features\n",
    "In order to **not** push any other aglorithm to the limit on the current data model, let's try to add some features that might help to classify tweets.\n",
    "\n",
    "A common sense suggest that special characters like exclamation marks and the casing might be important in the task of determining the sentiment.\n",
    "The following features will be added to the data model:\n",
    "\n",
    "| Feature name | Explanation |\n",
    "|------|------|\n",
    "|Number of uppercase | people tend to express with either positive or negative emotions by using A LOT OF UPPERCASE WORDS |\n",
    "| Number of ! | exclamation marks are likely to increase the strength of opinion |\n",
    "| Number of ? | might distinguish neutral tweets - seeking for information |\n",
    "| Number of positive emoticons | positive emoji will most likely not occur in the negative tweets|\n",
    "| Number of negative emoticons | inverse to the one above |\n",
    "| Number of ... | commonly used in commenting something |\n",
    "| Number of quotations | same as above | \n",
    "| Number of mentions | sometimes people put a lot of mentions on positive tweets, to share something good |\n",
    "| Number of hashtags | just for the experiment |\n",
    "| Number of urls | similiar to the number of mentions |\n",
    "\n",
    "Extraction of those features must be done before any preprocessing happens.\n",
    "\n",
    "For the purpose of emoticons, the ```EmoticonDetector``` class is created. The file ```emoticons.txt``` contains list of positive and negative emoticons, which are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "\n",
    "    def __init__(self, emoticon_file=\"data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "\n",
    "            self.emoticons[line] = positive\n",
    "\n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "\n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_ExtraFeatures(TwitterData_Wordlist):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_data.columns if col.startswith(\"number_of\")]\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + extra_columns + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"emotion\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            for _, col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_data.loc[idx, col])\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "\n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "\n",
    "            return counter\n",
    "\n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "\n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_data[\"text\"]))\n",
    "\n",
    "        # number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                             self.processed_data[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "\n",
    "        # number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                                self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "\n",
    "        # number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                             self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "\n",
    "        # number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                            self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "\n",
    "        # number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "\n",
    "        # number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "\n",
    "        # number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "\n",
    "        # number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                        self.processed_data[\"text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "\n",
    "        # number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt),\n",
    "                self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "\n",
    "        # number of negative emoticons\n",
    "        negative_emo = list(map(\n",
    "            lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt),\n",
    "            self.processed_data[\"splitted_text\"]))\n",
    "\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_data.loc[:, column_name] = pd.Series(column_content, index=self.processed_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>number_of_uppercase</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_urls</th>\n",
       "      <th>number_of_positive_emo</th>\n",
       "      <th>...</th>\n",
       "      <th>bill_bow</th>\n",
       "      <th>expens_bow</th>\n",
       "      <th>bank_bow</th>\n",
       "      <th>prob_bow</th>\n",
       "      <th>quotth_bow</th>\n",
       "      <th>honey_bow</th>\n",
       "      <th>choic_bow</th>\n",
       "      <th>group_bow</th>\n",
       "      <th>trek_bow</th>\n",
       "      <th>chri_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  number_of_uppercase  number_of_exclamation  number_of_question  \\\n",
       "0  negative                    1                      0                   0   \n",
       "1  negative                    2                      4                   0   \n",
       "2  negative                    1                      1                   0   \n",
       "3  negative                    3                      0                   0   \n",
       "4  negative                    2                      0                   0   \n",
       "\n",
       "   number_of_ellipsis  number_of_hashtags  number_of_mentions  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   1   \n",
       "2                   0                   0                   1   \n",
       "3                   0                   0                   1   \n",
       "4                   0                   0                   1   \n",
       "\n",
       "   number_of_quotes  number_of_urls  number_of_positive_emo    ...     \\\n",
       "0                 0               0                       0    ...      \n",
       "1                 0               0                       0    ...      \n",
       "2                 1               0                       0    ...      \n",
       "3                 1               0                       0    ...      \n",
       "4                 0               0                       0    ...      \n",
       "\n",
       "   bill_bow  expens_bow  bank_bow  prob_bow  quotth_bow  honey_bow  choic_bow  \\\n",
       "0         0           0         0         0           0          0          0   \n",
       "1         0           0         0         0           0          0          0   \n",
       "2         0           0         0         0           0          0          0   \n",
       "3         1           0         0         0           0          0          0   \n",
       "4         0           0         0         0           0          0          0   \n",
       "\n",
       "   group_bow  trek_bow  chri_bow  \n",
       "0          0         0         0  \n",
       "1          0         0         0  \n",
       "2          0         0         0  \n",
       "3          0         0         0  \n",
       "4          0         0         0  \n",
       "\n",
       "[5 rows x 999 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_ExtraFeatures()\n",
    "data.initialize(\"data/train16.csv\")\n",
    "data.build_features()\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.build_wordlist()\n",
    "data_model, labels = data.build_data_model()\n",
    "data_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic behind extra features\n",
    "Let's see how (some) of the extra features separate the data set. Some of them, i.e number exclamation marks, number of pos/neg emoticons do this really well. Despite of the good separation, those features sometimes occur only on small subset of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "a7fcb203-faf8-4083-8995-42f60d321d3f",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          705,
          437
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_positive_emo\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"565b2665-bcfb-4521-89fd-c825ec0efe23\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"565b2665-bcfb-4521-89fd-c825ec0efe23\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [705, 437], \"type\": \"bar\", \"uid\": \"3603f9d9-2aaa-4867-a449-ea7b1fae1f72\"}], {\"title\": \"How feature \\\"number_of_positive_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"565b2665-bcfb-4521-89fd-c825ec0efe23\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"565b2665-bcfb-4521-89fd-c825ec0efe23\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"565b2665-bcfb-4521-89fd-c825ec0efe23\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [705, 437], \"type\": \"bar\", \"uid\": \"3603f9d9-2aaa-4867-a449-ea7b1fae1f72\"}], {\"title\": \"How feature \\\"number_of_positive_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"565b2665-bcfb-4521-89fd-c825ec0efe23\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "ca68fef2-0a9a-4c9a-9116-b3adc2ac8c94",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          180,
          280
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_negative_emo\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"2c804eff-44f7-441e-b3e9-2e40c11ec128\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2c804eff-44f7-441e-b3e9-2e40c11ec128\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [180, 280], \"type\": \"bar\", \"uid\": \"13c0d80e-5c33-4a3e-a676-d93fd8e81a4a\"}], {\"title\": \"How feature \\\"number_of_negative_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"2c804eff-44f7-441e-b3e9-2e40c11ec128\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"2c804eff-44f7-441e-b3e9-2e40c11ec128\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2c804eff-44f7-441e-b3e9-2e40c11ec128\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [180, 280], \"type\": \"bar\", \"uid\": \"13c0d80e-5c33-4a3e-a676-d93fd8e81a4a\"}], {\"title\": \"How feature \\\"number_of_negative_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"2c804eff-44f7-441e-b3e9-2e40c11ec128\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "5214b515-f6ef-41d1-9fe7-221ba6df5a05",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          17696,
          12513
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_exclamation\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"d579f964-fec8-477f-a98f-e5161906847a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d579f964-fec8-477f-a98f-e5161906847a\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [17696, 12513], \"type\": \"bar\", \"uid\": \"e3d06a3a-3f02-48f4-b600-0c2770468e0a\"}], {\"title\": \"How feature \\\"number_of_exclamation\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"d579f964-fec8-477f-a98f-e5161906847a\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d579f964-fec8-477f-a98f-e5161906847a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d579f964-fec8-477f-a98f-e5161906847a\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [17696, 12513], \"type\": \"bar\", \"uid\": \"e3d06a3a-3f02-48f4-b600-0c2770468e0a\"}], {\"title\": \"How feature \\\"number_of_exclamation\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"d579f964-fec8-477f-a98f-e5161906847a\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "64ffc596-07b8-47f0-b03b-9b8ac19cbea0",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          1307,
          968
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_hashtags\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [1307, 968], \"type\": \"bar\", \"uid\": \"b39a1ac8-af78-426d-8518-614ad60d7523\"}], {\"title\": \"How feature \\\"number_of_hashtags\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [1307, 968], \"type\": \"bar\", \"uid\": \"b39a1ac8-af78-426d-8518-614ad60d7523\"}], {\"title\": \"How feature \\\"number_of_hashtags\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"1dcd9259-4e04-41f9-b3f5-3a57882857d7\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "type": "bar",
         "uid": "94d0336e-6b2b-44a3-b188-fb9879b17eef",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          5395,
          5032
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_question\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [5395, 5032], \"type\": \"bar\", \"uid\": \"c317ac5c-e487-4db0-953a-15861250b8a1\"}], {\"title\": \"How feature \\\"number_of_question\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\", [{\"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [5395, 5032], \"type\": \"bar\", \"uid\": \"c317ac5c-e487-4db0-953a-15861250b8a1\"}], {\"title\": \"How feature \\\"number_of_question\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"322b3ccf-c1d1-4f64-a3de-508545f0f48a\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiments = [\"positive\",\"negative\",\"neutral\"]\n",
    "plots_data_ef = []\n",
    "for what in map(lambda o: \"number_of_\"+o,[\"positive_emo\",\"negative_emo\",\"exclamation\",\"hashtags\",\"question\"]):\n",
    "    ef_grouped = data_model[data_model[what]>=1].groupby([\"label\"]).count()\n",
    "    plots_data_ef.append({\"data\":[graph_objs.Bar(\n",
    "            x = sentiments,\n",
    "            y = [ef_grouped.loc[s,:][0] for s in sentiments if s in ef_grouped.index],\n",
    "    )], \"title\":\"How feature \\\"\"+what+\"\\\" separates the tweets\"})\n",
    "    \n",
    "\n",
    "for plot_data_ef in plots_data_ef:\n",
    "    plotly.offline.iplot({\n",
    "            \"data\":plot_data_ef[\"data\"],\n",
    "            \"layout\":graph_objs.Layout(title=plot_data_ef[\"title\"])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: extended features + Random Forest\n",
    "As a second attempt on the classification the **Random Forest** will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Testing RandomForestClassifier\n",
      "Learing time 161.3562970161438s\n",
      "Predicting time 3.748042106628418s\n",
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.74481796 0.74304255]\n",
      "Precision[0.74225371 0.74563641]\n",
      "Recall   [0.7474     0.74046667]\n",
      "Accuracy 0.7439333333333333\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(random_state=seed,n_estimators=403,n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the initial split was lower than the one for the Naive Bayes, but let's see what happens during crossvalidation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 70.09595036506653s\n",
      "Accuracy: [ 0.54344624  0.50442478  0.37905605  0.27876106  0.37905605  0.52141802\n",
      "  0.5155096   0.55621302]\n",
      "Average accuracy: 0.459735602399\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks better, however it's still not much above accuracy of the random classifier and barely better than Naive Bayes classifier.\n",
    "\n",
    "We can observe a low recall level of the RandomForest classifier for the negative class, which may be caused by the data skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More features - word2vec\n",
    "The overall performance of the previous classifiers could be enhanced by performing time-consuming parameters adjustments, however there's not guarantee on how big the gain will be.\n",
    "\n",
    "If the out-of-the-shelf methods did not performed well, it seems that there's not much in the data itself. The next idea to add more into data model is to use word2vec representation of a tweet to perform classification. \n",
    "\n",
    "The word2vec allows to transform words into vectors of numbers. Those vectors represent abstract features, that describe the word similarities and relationships (i.e co-occurence).\n",
    "\n",
    "What is the best in the word2vec is that operations on the vectors approximately keep the characteristics of the words, so that joining (averaging) vectors from the words from sentence procude vector that is likely to represent the general topic of the sentence.\n",
    "\n",
    "A lot of pre-trained word2vec models exists, and some of them were trained on huge volumes of data. For the purpose of this analysis, the one trained on over **2 billion of tweets** with 200 dimensions (one vector consists of 200 numbers) is used.\n",
    "The pre-trained model can be downloaded here: https://github.com/3Top/word2vec-api\n",
    "\n",
    "\n",
    "### From GloVe to word2vec\n",
    "In order to use GloVe-trained model in ```gensim``` library, it needs to be converted to word2vec format. The only difference between those formats is that word2vec text files starts with two numbers: *number of lines in file* and *number of dimensions*. The file ```glove.twitter.27B.200d.txt``` does not contain those lines.\n",
    "\n",
    "Unfortunaltely, this text file size is over 1.9GB and text editors cannot be used to open and modify it in reasonable amount of time, this **C#** snippet adds this required line (sorry that it's not Python, but I was having memory problems with encoding of the file in Python. It's required to use x64 target):\n",
    "```{Csharp}\n",
    "using (var fileStream = new FileStream(\"glove.twitter.27B.200d.txt\", FileMode.Open,FileAccess.ReadWrite))\n",
    "{\n",
    "    var lines = new LinkedList<string>();\n",
    "    using (var streamReader = new StreamReader(fileStream))\n",
    "    {\n",
    "        while (!streamReader.EndOfStream)\n",
    "        {\n",
    "            lines.AddLast(streamReader.ReadLine());\n",
    "        }\n",
    "    }\n",
    "    lines.AddFirst(\"1193514 200\");\n",
    "    File.WriteAllLines(\"word2vec.twitter.27B.200d.txt.txt\", lines);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Already modified GloVe for file\n",
    "The file that has the first line appended can be downloaded from here (622MB 7-zip file, ultra compression): https://marcin.egnyte.com/dl/gk9nRsVMMY\n",
    "\n",
    "### Using Word2Vec\n",
    "The following class exposes a easy to use interface over the word2vec API from ```gensim``` library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
       "  -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
       "   0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
       "   0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
       "   0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
       "  -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
       "  -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
       "   0.67204 ]\n",
       " [-0.41486   0.71848  -0.3045    0.87445   0.22441  -0.56488  -0.37566\n",
       "  -0.44801   0.61347  -0.11359   0.74556  -0.10598  -1.1882    0.50974\n",
       "   1.3511    0.069851  0.73314   0.26773  -1.1787   -0.148     0.039853\n",
       "   0.033107 -0.27406   0.25125   0.41507  -1.6188   -0.81778  -0.73892\n",
       "  -0.28997   0.57277   3.4719    0.73817  -0.044495 -0.15119  -0.93503\n",
       "  -0.13152  -0.28562   0.76327  -0.83332  -0.6793   -0.39099  -0.64466\n",
       "   1.0044   -0.2051    0.46799   0.99314  -0.16221  -0.46022  -0.37639\n",
       "  -0.67542 ]]\n",
       "<NDArray 2x50 @cpu(0)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "glove[[\"hello\", \"world\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecProvider(object):\n",
    "    word2vec = None\n",
    "    dimensions = 0\n",
    "\n",
    "    def load(self, path_to_word2vec):\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(path_to_word2vec, binary=False)\n",
    "        self.word2vec.init_sims(replace=True)\n",
    "        self.dimensions = self.word2vec.vector_size\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if word not in self.word2vec.vocab:\n",
    "            return None\n",
    "\n",
    "        return self.word2vec.syn0norm[self.word2vec.vocab[word].index]\n",
    "\n",
    "    def get_similarity(self, word1, word2):\n",
    "        if word1 not in self.word2vec.vocab or word2 not in self.word2vec.vocab:\n",
    "            return None\n",
    "\n",
    "        return self.word2vec.similarity(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecProvider()\n",
    "model_dir = path.join(dataset_dir, \"glove.twitter.27B/glove.twitter.27B/glove.twitter.27B.25d.txt\")\n",
    "# REPLACE PATH TO THE FILE\n",
    "word2vec.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra features from word2vec\n",
    "Besides the 200 additional features from the word2vec representation, I had an idea of 3 more features. If word2vec allows to find similarity between words, that means it can find similarity to the specific emotion-representing words. The first idea was to compute similarity of the whole tweet with words from labels: *positive, negative, neutral*. Since the purpose was to find the sentiment, I thought that it will be better to find similarity with more expressive words such as: **good** and **bad**. For the neutral sentiment, I've used word **information**, since most of the tweets with neutral sentiment were giving the information.\n",
    "\n",
    "The features were builded by computing **mean similarity of the whole tweet to the given word**. Then, those mean values were normalized to [0;1] in order to deal with different word count across tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data model\n",
    "The final data model will contain:\n",
    "* extra text features (number of: !, ?, :-) etc)\n",
    "* word2vec similarity to \"good\", \"bad\" and \"information\" words\n",
    "* word2vec 200 dimension averaged representation of a tweet\n",
    "* bag-of-word representation of a tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData(TwitterData_ExtraFeatures):\n",
    "    \n",
    "    def build_final_model(self, word2vec_provider, stopwords=nltk.corpus.stopwords.words(\"english\")):\n",
    "        whitelist = self.whitelist\n",
    "        stopwords = list(filter(lambda sw: sw not in whitelist, stopwords))\n",
    "        extra_columns = [col for col in self.processed_data.columns if col.startswith(\"number_of\")]\n",
    "        similarity_columns = [\"bad_similarity\", \"good_similarity\", \"information_similarity\"]\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + [\"original_id\"] + extra_columns + similarity_columns + list(\n",
    "            map(lambda i: \"word2vec_{0}\".format(i), range(0, word2vec_provider.dimensions))) + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"emotion\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            current_row.append(self.processed_data.loc[idx, \"id\"])\n",
    "\n",
    "            for _, col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_data.loc[idx, col])\n",
    "\n",
    "            # average similarities with words\n",
    "            tokens = self.processed_data.loc[idx, \"tokenized_text\"]\n",
    "            for main_word in map(lambda w: w.split(\"_\")[0], similarity_columns):\n",
    "                current_similarities = [abs(sim) for sim in\n",
    "                                        map(lambda word: word2vec_provider.get_similarity(main_word, word.lower()), tokens) if\n",
    "                                        sim is not None]\n",
    "                if len(current_similarities) <= 1:\n",
    "                    current_row.append(0 if len(current_similarities) == 0 else current_similarities[0])\n",
    "                    continue\n",
    "                max_sim = max(current_similarities)\n",
    "                min_sim = min(current_similarities)\n",
    "                if max_sim == min_sim:\n",
    "                    current_row.append(max_sim)\n",
    "                    continue\n",
    "                current_similarities = [((sim - min_sim) / (max_sim - min_sim)) for sim in\n",
    "                                        current_similarities]  # normalize to <0;1>\n",
    "                current_row.append(np.array(current_similarities).mean())\n",
    "\n",
    "            # add word2vec vector\n",
    "            tokens = self.processed_data.loc[idx, \"tokenized_text\"]\n",
    "            current_word2vec = []\n",
    "            for _, word in enumerate(tokens):\n",
    "                vec = word2vec_provider.get_vector(word.lower())\n",
    "                if vec is not None:\n",
    "                    current_word2vec.append(vec)\n",
    "\n",
    "            averaged_word2vec = np.array(current_word2vec).mean(axis=0).tolist()\n",
    "            if np.any(np.isnan(averaged_word2vec)):\n",
    "                averaged_word2vec = np.zeros((word2vec_provider.dimensions,)).tolist()\n",
    "                \n",
    "            current_row += averaged_word2vec\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TwitterData()\n",
    "td.initialize(\"data/train16.csv\")\n",
    "td.build_features()\n",
    "td.cleanup(TwitterCleanuper())\n",
    "td.tokenize()\n",
    "td.stem()\n",
    "td.build_wordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.build_final_model(word2vec)\n",
    "\n",
    "td.data_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = td.data_model\n",
    "data_model.drop(\"original_id\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "data_before_word_gen = data_model\n",
    "% store data_before_after_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic behind word2vec extra features\n",
    "In order to show, why the 3 extra features built using word2vec might help in sentiment analysis. The chart below shows how many tweets from given category were dominating (had highest value) on similarity to those words.\n",
    "Although the words doesn't seem to separate the sentiments themselves, the differences between them in addition to other parameters, may help the classification process - i.e when tweet has highest value on *good_similarity* it's more likely for it to be classified to have positive sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "name": "Number of dominating positive",
         "type": "bar",
         "uid": "96a01dd8-8d9e-4492-91e7-a1658602c6e3",
         "x": [
          "good",
          "bad",
          "information"
         ],
         "y": [
          21833,
          23366,
          8595
         ]
        },
        {
         "name": "Number of dominating negative",
         "type": "bar",
         "uid": "a1da27f9-2a99-45fb-a251-528f5d0a7d55",
         "x": [
          "good",
          "bad",
          "information"
         ],
         "y": [
          19772,
          24929,
          7776
         ]
        }
       ],
       "layout": {
        "title": "Number of tweets dominating on similarity to: good, bad, information"
       }
      },
      "text/html": [
       "<div id=\"217701c9-a4b3-4d1d-b8f5-74a99054710e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"217701c9-a4b3-4d1d-b8f5-74a99054710e\", [{\"name\": \"Number of dominating positive\", \"x\": [\"good\", \"bad\", \"information\"], \"y\": [21833, 23366, 8595], \"type\": \"bar\", \"uid\": \"017a8609-9f2e-4032-9209-d04dbab147c8\"}, {\"name\": \"Number of dominating negative\", \"x\": [\"good\", \"bad\", \"information\"], \"y\": [19772, 24929, 7776], \"type\": \"bar\", \"uid\": \"68c87059-f54b-466e-911f-484a93f1769d\"}], {\"title\": \"Number of tweets dominating on similarity to: good, bad, information\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"217701c9-a4b3-4d1d-b8f5-74a99054710e\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"217701c9-a4b3-4d1d-b8f5-74a99054710e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"217701c9-a4b3-4d1d-b8f5-74a99054710e\", [{\"name\": \"Number of dominating positive\", \"x\": [\"good\", \"bad\", \"information\"], \"y\": [21833, 23366, 8595], \"type\": \"bar\", \"uid\": \"017a8609-9f2e-4032-9209-d04dbab147c8\"}, {\"name\": \"Number of dominating negative\", \"x\": [\"good\", \"bad\", \"information\"], \"y\": [19772, 24929, 7776], \"type\": \"bar\", \"uid\": \"68c87059-f54b-466e-911f-484a93f1769d\"}], {\"title\": \"Number of tweets dominating on similarity to: good, bad, information\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"217701c9-a4b3-4d1d-b8f5-74a99054710e\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_plot = [\"bad_similarity\", \"good_similarity\", \"information_similarity\"]\n",
    "bad, good, info = columns_to_plot\n",
    "sentiments = [\"positive\",\"negative\",\"neutral\"]\n",
    "only_positive = data_model[data_model[good]>=data_model[bad]]\n",
    "only_positive = only_positive[only_positive[good]>=only_positive[info]].groupby([\"label\"]).count()\n",
    "\n",
    "only_negative = data_model[data_model[bad] >= data_model[good]]\n",
    "only_negative = only_negative[only_negative[bad] >= only_negative[info]].groupby([\"label\"]).count()\n",
    "\n",
    "only_info = data_model[data_model[info]>=data_model[good]]\n",
    "only_info = only_info[only_info[info]>=only_info[bad]].groupby([\"label\"]).count()\n",
    "\n",
    "plot_data_w2v = []\n",
    "for sentiment in sentiments:\n",
    "    if sentiment not in grouped.index:\n",
    "        continue\n",
    "    plot_data_w2v.append(graph_objs.Bar(\n",
    "            x = [\"good\",\"bad\", \"information\"],\n",
    "            y = [only_positive.loc[sentiment,:][0], only_negative.loc[sentiment,:][0], only_info.loc[sentiment,:][0]],\n",
    "            name = \"Number of dominating \" + sentiment\n",
    "    ))\n",
    "    \n",
    "plotly.offline.iplot({\n",
    "        \"data\":plot_data_w2v,\n",
    "        \"layout\":graph_objs.Layout(title=\"Number of tweets dominating on similarity to: good, bad, information\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: full model + Random Forest\n",
    "The model is now complete. With a lot of new features, the learning algorithms should perform totally differently on the new data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Testing RandomForestClassifier\n",
      "Learing time 66.11012649536133s\n",
      "Predicting time 1.482553482055664s\n",
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.75139878 0.74797785]\n",
      "Precision[0.74633344 0.75315985]\n",
      "Recall   [0.75653333 0.74286667]\n",
      "Accuracy 0.7497\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "classifier = RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed)\n",
    "\n",
    "model = classifier.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance for Random Forest model (like xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_22 0.044264489130073637\n",
      "word2vec_19 0.03516557381575901\n",
      "word2vec_9 0.032555193630619386\n",
      "word2vec_21 0.027603438947028155\n",
      "word2vec_17 0.026433948439632773\n",
      "word2vec_15 0.024704563142900673\n",
      "word2vec_20 0.024522982662411614\n",
      "word2vec_7 0.023312459786694245\n",
      "word2vec_0 0.02295057240039318\n",
      "word2vec_14 0.022262212809801164\n",
      "word2vec_18 0.02219127241878715\n",
      "word2vec_2 0.020867482191840776\n",
      "word2vec_5 0.020805903011582673\n",
      "word2vec_6 0.020658312254964617\n",
      "word2vec_3 0.0202814454020409\n",
      "word2vec_10 0.020214788342514518\n",
      "word2vec_24 0.02011606374765884\n",
      "word2vec_8 0.019804521821893836\n",
      "word2vec_23 0.019731531130739662\n",
      "word2vec_1 0.0196932258434081\n",
      "word2vec_13 0.019498597812885408\n",
      "word2vec_12 0.01933118592441679\n",
      "word2vec_11 0.019238055086251822\n",
      "word2vec_4 0.019176764597854692\n",
      "word2vec_16 0.019059604228721508\n",
      "bad_similarity 0.0184155080425661\n",
      "information_similarity 0.018396770810620622\n",
      "good_similarity 0.018277185962704456\n",
      "number_of_mentions 0.01737036448624118\n",
      "number_of_exclamation 0.00998810367246514\n",
      "number_of_uppercase 0.009057917243209437\n",
      "miss_bow 0.008848980558640406\n",
      "sad_bow 0.006575706964384843\n",
      "thank_bow 0.0051633207192666114\n",
      "not_bow 0.005058237903192976\n",
      "love_bow 0.004572123076027212\n",
      "number_of_ellipsis 0.004129943766690775\n",
      "number_of_question 0.0035863983983468787\n",
      "wish_bow 0.0035477711590953124\n",
      "work_bow 0.003350457074464625\n",
      "good_bow 0.0033488458007297213\n",
      "sorri_bow 0.0033119528853344923\n",
      "want_bow 0.0027503740881029432\n",
      "dont_bow 0.0025776426742511534\n",
      "cant_bow 0.0023738880090491163\n",
      "number_of_urls 0.002280695245500226\n",
      "im_bow 0.0021990274550016506\n",
      "hate_bow 0.0021920200611806654\n",
      "feel_bow 0.002005059881378336\n",
      "go_bow 0.001988863934297056\n",
      "number_of_quotes 0.0019370790958871456\n",
      "bad_bow 0.0019085264254774456\n",
      "lol_bow 0.0018943424175236648\n",
      "get_bow 0.0017278154687491408\n",
      "wa_bow 0.0016939442277553382\n",
      "thi_bow 0.0015874416735015794\n",
      "haha_bow 0.0015148486626318364\n",
      "day_bow 0.0015091940074482685\n",
      "great_bow 0.001493476448187795\n",
      "didnt_bow 0.001485961368546292\n",
      "like_bow 0.001455281179168426\n",
      "whi_bow 0.0014502965847482219\n",
      "suck_bow 0.0014380885570498864\n",
      "got_bow 0.0013629490346741843\n",
      "time_bow 0.001362369123489231\n",
      "sick_bow 0.0013438099622505484\n",
      "still_bow 0.001343327390257747\n",
      "need_bow 0.0012989859447786528\n",
      "happi_bow 0.0012827387837398296\n",
      "one_bow 0.0012541772653663068\n",
      "today_bow 0.0012514125103187603\n",
      "wait_bow 0.0012480963826053408\n",
      "back_bow 0.0012320060307190802\n",
      "see_bow 0.0011996733002661052\n",
      "hurt_bow 0.0011951760362401325\n",
      "watch_bow 0.0011797343720106207\n",
      "think_bow 0.0011649492264018781\n",
      "know_bow 0.001154674702179353\n",
      "last_bow 0.0011416714801092505\n",
      "well_bow 0.0011227779669366858\n",
      "realli_bow 0.0011175045316228545\n",
      "number_of_hashtags 0.0011060936803855097\n",
      "onli_bow 0.0010711036526205794\n",
      "hope_bow 0.0010434497798622736\n",
      "new_bow 0.0010289290438679505\n",
      "make_bow 0.0010101320602791584\n",
      "oh_bow 0.0010079247601746295\n",
      "ill_bow 0.0009875619186936904\n",
      "twitter_bow 0.0009841230635393512\n",
      "home_bow 0.000983932898396136\n",
      "though_bow 0.0009766400335815405\n",
      "come_bow 0.0009744849570715394\n",
      "lost_bow 0.0009722312518180818\n",
      "ha_bow 0.0009622549389558986\n",
      "u_bow 0.0009571270808669659\n",
      "poor_bow 0.0009500936440032555\n",
      "excit_bow 0.0009380868143836651\n",
      "night_bow 0.0009348061587604595\n",
      "much_bow 0.0009258375649952924\n",
      "look_bow 0.0009217442805749948\n",
      "wont_bow 0.0009161145804587027\n",
      "awesom_bow 0.0009126401251703303\n",
      "fun_bow 0.0009022473729891163\n",
      "na_bow 0.000892166013318456\n",
      "better_bow 0.000888448337030155\n",
      "doesnt_bow 0.0008841414313818174\n",
      "nice_bow 0.0008802597665604743\n",
      "glad_bow 0.0008288915425718145\n",
      "tomorrow_bow 0.0008282423849945552\n",
      "right_bow 0.000821897968528931\n",
      "amp_bow 0.0008205982502652837\n",
      "say_bow 0.0008163472947539442\n",
      "rain_bow 0.0007911158399808803\n",
      "bore_bow 0.0007775423724040798\n",
      "yeah_bow 0.0007731701068497342\n",
      "follow_bow 0.0007691174159375315\n",
      "thing_bow 0.0007473177819399979\n",
      "tri_bow 0.0007396004561214698\n",
      "week_bow 0.0007346787312234597\n",
      "ye_bow 0.0007255824603992624\n",
      "morn_bow 0.0007205471413251814\n",
      "never_bow 0.0007089131320245492\n",
      "would_bow 0.0007075699558468976\n",
      "final_bow 0.0006973945361239685\n",
      "number_of_positive_emo 0.0006964076851185248\n",
      "way_bow 0.0006907211707334267\n",
      "sleep_bow 0.0006906497788013598\n",
      "yay_bow 0.0006902563570184029\n",
      "could_bow 0.0006825011142202249\n",
      "tonight_bow 0.0006734324765591081\n",
      "hi_bow 0.0006709090643216783\n",
      "friend_bow 0.0006588170531201464\n",
      "let_bow 0.0006503574955928693\n",
      "take_bow 0.0006497106657708076\n",
      "even_bow 0.0006441312235159893\n",
      "wan_bow 0.0006422475000966197\n",
      "start_bow 0.0006401996642814858\n",
      "hour_bow 0.0006337101589149621\n",
      "leav_bow 0.0006318147982065047\n",
      "done_bow 0.0006216210764941752\n",
      "ive_bow 0.0006177632055985831\n",
      "damn_bow 0.0006173280281715776\n",
      "hey_bow 0.0006115595107860215\n",
      "enjoy_bow 0.000610583471873946\n",
      "sure_bow 0.0006100476645128864\n",
      "find_bow 0.0005922850357132976\n",
      "veri_bow 0.0005895038277538161\n",
      "bed_bow 0.0005847050209828105\n",
      "tweet_bow 0.0005841861401952835\n",
      "play_bow 0.000582736728400019\n",
      "gon_bow 0.0005765549218406797\n",
      "school_bow 0.0005709414820911071\n",
      "peopl_bow 0.0005650963535804428\n",
      "talk_bow 0.0005532465845221966\n",
      "next_bow 0.0005527982101118926\n",
      "show_bow 0.0005520602444927195\n",
      "ugh_bow 0.0005506179154834246\n",
      "soon_bow 0.0005474983147576776\n",
      "ok_bow 0.0005460272158957062\n",
      "long_bow 0.0005419249065786433\n",
      "everyon_bow 0.0005349072163534827\n",
      "littl_bow 0.0005273819268467575\n",
      "lt_bow 0.0005268173660265\n",
      "guy_bow 0.0005267751281644551\n",
      "amaz_bow 0.0005234713503964955\n",
      "ani_bow 0.0005223113264979175\n",
      "best_bow 0.000521085571898384\n",
      "first_bow 0.0005190705236034684\n",
      "listen_bow 0.0005168361444331305\n",
      "weekend_bow 0.0005134404516618755\n",
      "use_bow 0.0005055779017837196\n",
      "movi_bow 0.0005048864256667957\n",
      "cri_bow 0.0005021239058069091\n",
      "isnt_bow 0.0005016037682905242\n",
      "life_bow 0.0004961749326811162\n",
      "cool_bow 0.0004956721840563077\n",
      "die_bow 0.000494258884369453\n",
      "year_bow 0.00048811761803336064\n",
      "tire_bow 0.00048425588933031014\n",
      "song_bow 0.00048399576036591615\n",
      "girl_bow 0.0004823107955847873\n",
      "lot_bow 0.0004783152081364106\n",
      "alway_bow 0.00047584174095253025\n",
      "someth_bow 0.00047342402248561175\n",
      "call_bow 0.00047182143712860565\n",
      "anoth_bow 0.00046911636227240386\n",
      "left_bow 0.0004637053039350354\n",
      "read_bow 0.0004630850082344502\n",
      "bit_bow 0.0004619454334116543\n",
      "yet_bow 0.0004605792653258431\n",
      "alreadi_bow 0.00045722720638742336\n",
      "later_bow 0.0004566030916582378\n",
      "befor_bow 0.0004566023784731428\n",
      "guess_bow 0.00045576770131832594\n",
      "made_bow 0.0004549834022971861\n",
      "funni_bow 0.00045340301023731936\n",
      "away_bow 0.0004491677326753638\n",
      "mayb_bow 0.0004485890960033327\n",
      "pleas_bow 0.00044743189905954234\n",
      "keep_bow 0.00044679413870283354\n",
      "went_bow 0.000441300271990378\n",
      "aww_bow 0.0004403250891829134\n",
      "head_bow 0.0004385442894989895\n",
      "becaus_bow 0.00043508820383665944\n",
      "hous_bow 0.00043283387312684885\n",
      "mean_bow 0.00043147167875008386\n",
      "exam_bow 0.0004274195130497016\n",
      "live_bow 0.0004271986925910286\n",
      "eat_bow 0.0004246944905686304\n",
      "phone_bow 0.00042322912139996404\n",
      "fail_bow 0.00042085190862214614\n",
      "readi_bow 0.0004163634699014637\n",
      "man_bow 0.00041560146506789086\n",
      "x_bow 0.0004131380168258335\n",
      "pretti_bow 0.00041137106358758106\n",
      "hear_bow 0.0004084236480826347\n",
      "havent_bow 0.00040811856859526816\n",
      "least_bow 0.00040714481649308133\n",
      "help_bow 0.0004064317329160102\n",
      "us_bow 0.0004036602363255886\n",
      "tell_bow 0.00040349187544005396\n",
      "noth_bow 0.0004014262651366183\n",
      "old_bow 0.0003997253070165858\n",
      "headach_bow 0.0003982444786911745\n",
      "someon_bow 0.0003962725064066085\n",
      "cute_bow 0.000391561826534862\n",
      "gone_bow 0.0003878396601628484\n",
      "kid_bow 0.00038658914308104897\n",
      "sound_bow 0.00038639242144152924\n",
      "hot_bow 0.0003862609088787706\n",
      "check_bow 0.0003858021455143324\n",
      "happen_bow 0.00038481046679494284\n",
      "thought_bow 0.00038441658965699684\n",
      "finish_bow 0.00038344277222121265\n",
      "ya_bow 0.0003834062026951081\n",
      "summer_bow 0.0003774817951836596\n",
      "give_bow 0.00037291974997747263\n",
      "anymor_bow 0.0003682021572806936\n",
      "game_bow 0.0003627205958423717\n",
      "hahaha_bow 0.00036246201406066054\n",
      "boo_bow 0.0003624031400578012\n",
      "ever_bow 0.0003609898779460484\n",
      "okay_bow 0.00035917270713369166\n",
      "rip_bow 0.000358421578713839\n",
      "late_bow 0.000358367874275312\n",
      "might_bow 0.0003561754327336314\n",
      "ur_bow 0.0003560201614028612\n",
      "big_bow 0.0003551772041288806\n",
      "put_bow 0.0003542147089432439\n",
      "end_bow 0.0003524495159456138\n",
      "worri_bow 0.00035178268152440443\n",
      "omg_bow 0.00035158591393700763\n",
      "actual_bow 0.0003487478698639942\n",
      "earli_bow 0.0003440230525210942\n",
      "sun_bow 0.00034368440321984366\n",
      "parti_bow 0.00034254516731222276\n",
      "around_bow 0.00034242502278676454\n",
      "wow_bow 0.0003418469574143653\n",
      "world_bow 0.00034053116971585057\n",
      "stop_bow 0.00033987195364386114\n",
      "studi_bow 0.00033852218523385496\n",
      "welcom_bow 0.0003363509961577371\n",
      "stuff_bow 0.0003328478862327819\n",
      "said_bow 0.00033085933561103074\n",
      "car_bow 0.00032965257385228995\n",
      "yesterday_bow 0.0003280152100578625\n",
      "meet_bow 0.0003271282813039304\n",
      "aw_bow 0.0003269139753879277\n",
      "free_bow 0.0003259123702395643\n",
      "cold_bow 0.0003250539827627794\n",
      "wonder_bow 0.0003245853212618148\n",
      "babi_bow 0.0003242750743621499\n",
      "tho_bow 0.0003236844843810047\n",
      "id_bow 0.00032144261414097475\n",
      "move_bow 0.0003189189389536693\n",
      "run_bow 0.00031878828369185877\n",
      "weather_bow 0.0003183975802439733\n",
      "without_bow 0.00031838972976929396\n",
      "doe_bow 0.00031798334466328184\n",
      "also_bow 0.00031590256857420384\n",
      "must_bow 0.0003156049329908521\n",
      "number_of_negative_emo 0.0003144846469839633\n",
      "boy_bow 0.0003143637414182213\n",
      "beauti_bow 0.0003131884478887008\n",
      "monday_bow 0.00031212064666062336\n",
      "hard_bow 0.0003100803994472413\n",
      "wasnt_bow 0.00030904968833287533\n",
      "found_bow 0.0003086155443732122\n",
      "two_bow 0.0003071555185227351\n",
      "saw_bow 0.0003066183463578153\n",
      "walk_bow 0.0003053214818611295\n",
      "n_bow 0.0003049934119141793\n",
      "stupid_bow 0.00030488760549481645\n",
      "fuck_bow 0.00030370709466468366\n",
      "shop_bow 0.00030219139843833587\n",
      "smile_bow 0.0002949405566130169\n",
      "wrong_bow 0.0002947032021466205\n",
      "seem_bow 0.00029319603917663537\n",
      "pic_bow 0.00029258411027860957\n",
      "busi_bow 0.0002911079600415921\n",
      "mani_bow 0.0002883503385039336\n",
      "buy_bow 0.00028800118896988544\n",
      "far_bow 0.00028759544543684027\n",
      "job_bow 0.00028566907172387766\n",
      "ta_bow 0.00028497577336413096\n",
      "quit_bow 0.0002843410761878647\n",
      "music_bow 0.0002842473852228304\n",
      "stay_bow 0.0002840215116077344\n",
      "sinc_bow 0.00028271523151613624\n",
      "hair_bow 0.00028261715003311547\n",
      "clean_bow 0.0002802492462349434\n",
      "mom_bow 0.0002802238751817895\n",
      "iphon_bow 0.0002783583572113833\n",
      "friday_bow 0.00027791291963742584\n",
      "total_bow 0.00027693056676190406\n",
      "famili_bow 0.0002753150942045135\n",
      "almost_bow 0.0002748352061711841\n",
      "turn_bow 0.0002745965575258815\n",
      "god_bow 0.00027157334111897134\n",
      "couldnt_bow 0.00027147848695052745\n",
      "pictur_bow 0.0002712648718864859\n",
      "anyth_bow 0.00027075430982527763\n",
      "everyth_bow 0.0002702669696277127\n",
      "sweet_bow 0.00027019850049860966\n",
      "hang_bow 0.0002695599177268915\n",
      "anyon_bow 0.0002688143778956436\n",
      "luck_bow 0.00026845889382359566\n",
      "enough_bow 0.0002678356924240031\n",
      "sunday_bow 0.0002672812431057198\n",
      "drive_bow 0.00026645610114923703\n",
      "break_bow 0.00026591036776006915\n",
      "unfortun_bow 0.0002654127126082974\n",
      "shower_bow 0.00026382180397301546\n",
      "birthday_bow 0.000262949822461074\n",
      "month_bow 0.0002627931554443716\n",
      "rememb_bow 0.00026230699957298457\n",
      "kill_bow 0.00026218373527868953\n",
      "dad_bow 0.0002616003246211898\n",
      "lucki_bow 0.0002605650081240725\n",
      "may_bow 0.0002597905579734597\n",
      "send_bow 0.00025967964403032724\n",
      "hit_bow 0.00025806899912645206\n",
      "lunch_bow 0.0002579076066508639\n",
      "seen_bow 0.00025769292766677013\n",
      "place_bow 0.0002563399930488738\n",
      "woke_bow 0.0002562125863117429\n",
      "video_bow 0.00025560522666104557\n",
      "believ_bow 0.0002555515888822889\n",
      "stuck_bow 0.0002515869503971521\n",
      "saturday_bow 0.0002509167479987636\n",
      "rest_bow 0.00025089801267093515\n",
      "mine_bow 0.0002506663304076099\n",
      "goodnight_bow 0.000250468944755877\n",
      "chang_bow 0.00024951220026246776\n",
      "till_bow 0.0002483578815264015\n",
      "name_bow 0.0002482747950763057\n",
      "book_bow 0.0002480890757171095\n",
      "idea_bow 0.0002478048782056972\n",
      "updat_bow 0.00024771912311053964\n",
      "fine_bow 0.00024666134675218167\n",
      "money_bow 0.00024656938043637807\n",
      "post_bow 0.0002463388066389924\n",
      "problem_bow 0.00024521378365652464\n",
      "hehe_bow 0.00024442307934133094\n",
      "forgot_bow 0.0002439428048809205\n",
      "real_bow 0.0002438528089438224\n",
      "anyway_bow 0.00024354156799667308\n",
      "plan_bow 0.00024281760448029426\n",
      "dinner_bow 0.00023993739377556472\n",
      "dog_bow 0.00023947121659991752\n",
      "sit_bow 0.00023891215968988648\n",
      "caus_bow 0.00023778921471972323\n",
      "open_bow 0.00023744187027326447\n",
      "coffe_bow 0.0002372591797655637\n",
      "dream_bow 0.00023616688054785474\n",
      "outsid_bow 0.0002361072793271597\n",
      "laugh_bow 0.0002355903687482683\n",
      "everi_bow 0.00023497849400900482\n",
      "room_bow 0.00023405167151481244\n",
      "shit_bow 0.00023360710521106955\n",
      "father_bow 0.00023318887230702634\n",
      "win_bow 0.0002325624121204233\n",
      "probabl_bow 0.00022889229665791003\n",
      "sadli_bow 0.00022805119376758778\n",
      "food_bow 0.0002268289088392741\n",
      "mind_bow 0.00022630168703966408\n",
      "drink_bow 0.00022544525253131184\n",
      "danc_bow 0.00022542097252011361\n",
      "write_bow 0.00022485590259379526\n",
      "close_bow 0.00022380028141018315\n",
      "true_bow 0.0002233847475412275\n",
      "wake_bow 0.0002230960876512886\n",
      "alon_bow 0.00022263613071978123\n",
      "onc_bow 0.00022260308240478477\n",
      "crazi_bow 0.00022127588003835528\n",
      "ask_bow 0.00022121930306488014\n",
      "sore_bow 0.00022053078361961996\n",
      "class_bow 0.00021911042770121438\n",
      "els_bow 0.00021848131867300758\n",
      "took_bow 0.00021822609832269183\n",
      "broke_bow 0.0002175456327104066\n",
      "minut_bow 0.00021720760792384288\n",
      "rock_bow 0.00021626497067580566\n",
      "text_bow 0.00021618983984329526\n",
      "xx_bow 0.00021514506000170765\n",
      "tv_bow 0.0002136791606551683\n",
      "w_bow 0.00021302025554724297\n",
      "abl_bow 0.00021156885467045968\n",
      "person_bow 0.00021116547094465236\n",
      "fan_bow 0.0002109724191364249\n",
      "pain_bow 0.0002091888006197416\n",
      "forward_bow 0.00020790054307450015\n",
      "care_bow 0.00020680642566424574\n",
      "catch_bow 0.00020664755997070418\n",
      "either_bow 0.00020630057271685783\n",
      "ah_bow 0.00020396802196484076\n",
      "offic_bow 0.00020303564186552005\n",
      "homework_bow 0.00020269295429843506\n",
      "shame_bow 0.0002014098091887662\n",
      "r_bow 0.00019981804548940257\n",
      "b_bow 0.00019952442784603954\n",
      "word_bow 0.00019942795591266707\n",
      "crap_bow 0.00019936829043981592\n",
      "sometim_bow 0.00019904046317599064\n",
      "part_bow 0.000198238917915826\n",
      "hell_bow 0.00019806546942409512\n",
      "awww_bow 0.00019773415259928783\n",
      "full_bow 0.0001960390257133393\n",
      "kinda_bow 0.0001958836737144689\n",
      "news_bow 0.00019556434452259002\n",
      "repli_bow 0.0001954673189370652\n",
      "train_bow 0.00019546723873132603\n",
      "beach_bow 0.0001941640474802869\n",
      "bring_bow 0.00019409785284207754\n",
      "theyr_bow 0.00019350397017083173\n",
      "sign_bow 0.00019015616175820942\n",
      "suppos_bow 0.00018976992326871374\n",
      "brother_bow 0.00018929939745628797\n",
      "test_bow 0.00018904525375166458\n",
      "goe_bow 0.00018867995023614357\n",
      "revis_bow 0.00018837318449467923\n",
      "interest_bow 0.0001876503714659163\n",
      "eye_bow 0.0001868030374968172\n",
      "raini_bow 0.00018646769381907357\n",
      "spend_bow 0.00018641757874554528\n",
      "jealou_bow 0.0001854155062385422\n",
      "relax_bow 0.00018507718616446515\n",
      "mad_bow 0.00018478530013821102\n",
      "set_bow 0.00018421616076789852\n",
      "hug_bow 0.00018382491115926324\n",
      "lmao_bow 0.00018374159242382692\n",
      "ago_bow 0.00018310559827862307\n",
      "email_bow 0.00018202620472916982\n",
      "youll_bow 0.00018201464023092053\n",
      "sing_bow 0.00018138429743864078\n",
      "yea_bow 0.00018087828071516537\n",
      "comput_bow 0.00018029840244611044\n",
      "definit_bow 0.00018012874915225525\n",
      "season_bow 0.00018009934868081255\n",
      "pick_bow 0.0001791296684973377\n",
      "half_bow 0.00017893884609256522\n",
      "trip_bow 0.00017805103146351791\n",
      "link_bow 0.00017781596781591026\n",
      "heart_bow 0.00017720929028848076\n",
      "disappoint_bow 0.00017716281569614273\n",
      "sister_bow 0.0001767419951507592\n",
      "ass_bow 0.00017670227058280566\n",
      "btw_bow 0.000176557121882885\n",
      "photo_bow 0.00017652453117354117\n",
      "page_bow 0.00017641570288308442\n",
      "came_bow 0.00017601504259441808\n",
      "add_bow 0.00017536222136492398\n",
      "onlin_bow 0.00017284061681743703\n",
      "soo_bow 0.0001727498691956208\n",
      "figur_bow 0.00017190263257227368\n",
      "cuz_bow 0.00017189047961327258\n",
      "bought_bow 0.00017140715095214002\n",
      "moment_bow 0.0001706849261657921\n",
      "messag_bow 0.0001706363012134787\n",
      "concert_bow 0.00017057679413076882\n",
      "short_bow 0.0001702917854394373\n",
      "forget_bow 0.0001692938281757525\n",
      "dear_bow 0.00016917395365937153\n",
      "super_bow 0.00016878395119693338\n",
      "bye_bow 0.00016866049199262023\n",
      "pass_bow 0.0001683398957129154\n",
      "broken_bow 0.00016795451499526496\n",
      "hand_bow 0.0001678454637908172\n",
      "whole_bow 0.00016782947852238972\n",
      "ticket_bow 0.00016782646310054897\n",
      "join_bow 0.00016715215218220938\n",
      "aint_bow 0.00016702260707764013\n",
      "blog_bow 0.00016609685074864193\n",
      "cours_bow 0.00016546873357722197\n",
      "decid_bow 0.00016529989870303491\n",
      "dude_bow 0.0001652106621446785\n",
      "account_bow 0.00016400033082598835\n",
      "heard_bow 0.00016382871926786944\n",
      "red_bow 0.00016328376737749864\n",
      "xd_bow 0.00016244349111980616\n",
      "fix_bow 0.00016231639519915744\n",
      "kind_bow 0.0001620607765720921\n",
      "face_bow 0.0001618451108822544\n",
      "cancel_bow 0.0001615791382892256\n",
      "wouldnt_bow 0.00016013379016598106\n",
      "breakfast_bow 0.00015982274372988324\n",
      "instead_bow 0.00015840947880109237\n",
      "learn_bow 0.00015779315592299154\n",
      "agre_bow 0.00015777500768699535\n",
      "awak_bow 0.00015776764405939288\n",
      "arent_bow 0.00015712308469236971\n",
      "pay_bow 0.0001570737507093285\n",
      "goodby_bow 0.00015704247016848512\n",
      "pack_bow 0.00015669208016225346\n",
      "worst_bow 0.00015653353311745414\n",
      "told_bow 0.00015624782871326902\n",
      "goin_bow 0.00015621827672669932\n",
      "facebook_bow 0.00015581017171507444\n",
      "visit_bow 0.00015580483637747135\n",
      "park_bow 0.0001553056518454388\n",
      "citi_bow 0.00015527462852814162\n",
      "hello_bow 0.00015513091913588065\n",
      "save_bow 0.0001550390147422576\n",
      "ride_bow 0.00015387586185746432\n",
      "nope_bow 0.0001533260130736726\n",
      "internet_bow 0.00015317984107777118\n",
      "drop_bow 0.00015313263974887586\n",
      "togeth_bow 0.0001530119974924636\n",
      "fall_bow 0.00015214186609177244\n",
      "cat_bow 0.00015194202833677852\n",
      "offici_bow 0.00015151603781065085\n",
      "hungri_bow 0.00015145528681165387\n",
      "meant_bow 0.00015098457655824264\n",
      "reason_bow 0.0001502470697607007\n",
      "wear_bow 0.00014966981990724342\n",
      "horribl_bow 0.00014927479296454995\n",
      "gym_bow 0.0001487221019078423\n",
      "bout_bow 0.000148187686557011\n",
      "point_bow 0.00014803960774660858\n",
      "high_bow 0.00014684567124555417\n",
      "store_bow 0.00014673332517749148\n",
      "worth_bow 0.0001465843395220816\n",
      "serious_bow 0.00014646292769368165\n",
      "bless_bow 0.00014633025424817887\n",
      "sooo_bow 0.00014516253463839096\n",
      "air_bow 0.00014501340667115884\n",
      "stori_bow 0.0001449555409476554\n",
      "wed_bow 0.0001449400132742547\n",
      "la_bow 0.0001448969611146924\n",
      "church_bow 0.0001438742573653207\n",
      "nap_bow 0.00014374481008578668\n",
      "water_bow 0.00014331948755629017\n",
      "afternoon_bow 0.0001431858729387709\n",
      "ps_bow 0.00014271320685179875\n",
      "date_bow 0.00014223316785895153\n",
      "complet_bow 0.00014207611967982555\n",
      "top_bow 0.0001415034903236864\n",
      "gt_bow 0.00014128629923331303\n",
      "ladi_bow 0.00014055832814386843\n",
      "tour_bow 0.00014054119743588925\n",
      "usual_bow 0.00014024699650808087\n",
      "p_bow 0.0001398001546981131\n",
      "kick_bow 0.0001397383137310393\n",
      "town_bow 0.00013964436529843424\n",
      "til_bow 0.00013917794389234304\n",
      "airport_bow 0.0001390604527818016\n",
      "mother_bow 0.00013903029950595655\n",
      "favorit_bow 0.00013891115355338217\n",
      "star_bow 0.00013875423429116375\n",
      "depress_bow 0.00013859388291857734\n",
      "list_bow 0.00013849171939239933\n",
      "cut_bow 0.0001367842126666493\n",
      "less_bow 0.00013602370827240075\n",
      "sigh_bow 0.0001359029618777796\n",
      "weird_bow 0.0001356519112499558\n",
      "order_bow 0.00013549240844275862\n",
      "perfect_bow 0.00013507368561179318\n",
      "differ_bow 0.00013465106908129964\n",
      "holiday_bow 0.00013397775909850768\n",
      "site_bow 0.00013394814229665794\n",
      "except_bow 0.00013390619137573933\n",
      "dead_bow 0.00013381077462469639\n",
      "tuesday_bow 0.0001335217988396361\n",
      "itll_bow 0.0001333779300179253\n",
      "french_bow 0.00013280409422798036\n",
      "sat_bow 0.00013232718511083634\n",
      "min_bow 0.00013175385949798415\n",
      "vote_bow 0.00013159826946005344\n",
      "sunni_bow 0.0001315642168211672\n",
      "lose_bow 0.00013056154146603424\n",
      "fast_bow 0.0001302275780288452\n",
      "chanc_bow 0.00012969876163684805\n",
      "support_bow 0.00012955675750270118\n",
      "annoy_bow 0.00012944264597050388\n",
      "download_bow 0.0001290231632634299\n",
      "second_bow 0.00012893709365276998\n",
      "chill_bow 0.00012856678638455398\n",
      "understand_bow 0.00012850936313498208\n",
      "fb_bow 0.00012838921710902278\n",
      "ate_bow 0.00012764950152853306\n",
      "due_bow 0.00012726987384028497\n",
      "london_bow 0.00012700136553210219\n",
      "notic_bow 0.00012670238464159857\n",
      "camp_bow 0.00012610428414181678\n",
      "math_bow 0.00012592072444114597\n",
      "question_bow 0.00012577501814498436\n",
      "asleep_bow 0.0001256620236470325\n",
      "hospit_bow 0.00012518334201744693\n",
      "graduat_bow 0.0001249945174084886\n",
      "episod_bow 0.0001247413099151308\n",
      "knew_bow 0.00012454150490745088\n",
      "line_bow 0.00012396222074445202\n",
      "answer_bow 0.00012332407908231184\n",
      "pool_bow 0.00012325970565925794\n",
      "ahh_bow 0.0001224812928621409\n",
      "ice_bow 0.00012235669751729973\n",
      "possibl_bow 0.00012219347938359636\n",
      "yall_bow 0.00012206775674208247\n",
      "congrat_bow 0.00012184566413566326\n",
      "insid_bow 0.00012163767602524803\n",
      "english_bow 0.00012148452329349964\n",
      "fair_bow 0.00012122583439518053\n",
      "flight_bow 0.00012121528476339078\n",
      "side_bow 0.00012089936903118659\n",
      "spent_bow 0.00012077854872191333\n",
      "alright_bow 0.00012056243095544216\n",
      "longer_bow 0.00012040959414926843\n",
      "ipod_bow 0.00011994526596973201\n",
      "hold_bow 0.00011990505642414736\n",
      "laptop_bow 0.00011966920297116541\n",
      "soooo_bow 0.00011946925916702735\n",
      "window_bow 0.00011916333824709958\n",
      "colleg_bow 0.00011906351788455986\n",
      "coupl_bow 0.00011892710080475007\n",
      "project_bow 0.00011872228843681213\n",
      "appar_bow 0.00011864752447011854\n",
      "chocol_bow 0.00011858424071201387\n",
      "upset_bow 0.00011855485066675441\n",
      "number_bow 0.00011850275252801867\n",
      "blue_bow 0.0001183232448246534\n",
      "mac_bow 0.00011764970097914807\n",
      "mess_bow 0.0001175702169134603\n",
      "black_bow 0.00011708057402945648\n",
      "realiz_bow 0.00011679690819373593\n",
      "load_bow 0.00011634378921640796\n",
      "ppl_bow 0.00011632406356288738\n",
      "youv_bow 0.00011600206130819563\n",
      "chat_bow 0.00011583216306162511\n",
      "moon_bow 0.00011494870866470653\n",
      "forev_bow 0.00011488303912786065\n",
      "xxx_bow 0.00011401421475813223\n",
      "three_bow 0.00011369496631856664\n",
      "dress_bow 0.0001136891798637099\n",
      "cake_bow 0.00011346258916592063\n",
      "beer_bow 0.0001134473733622352\n",
      "parent_bow 0.00011337316688058192\n",
      "album_bow 0.00011325038642461494\n",
      "hmm_bow 0.00011322117775445983\n",
      "cook_bow 0.00011258036597862423\n",
      "age_bow 0.00011253676930315659\n",
      "shall_bow 0.00011236049422844741\n",
      "rather_bow 0.00011201940744903778\n",
      "shoe_bow 0.00011169517527023673\n",
      "team_bow 0.00011150796748372259\n",
      "band_bow 0.00011146114579789861\n",
      "manag_bow 0.00011126736205981434\n",
      "record_bow 0.00011115855609742207\n",
      "lil_bow 0.00011098643274284707\n",
      "bet_bow 0.00011092603155507179\n",
      "thursday_bow 0.00011076885582113169\n",
      "return_bow 0.00011025235148026402\n",
      "yep_bow 0.00010965293727439723\n",
      "mr_bow 0.00010923342873225472\n",
      "mood_bow 0.0001091236798381102\n",
      "tea_bow 0.00010905200852245696\n",
      "card_bow 0.00010904699370555771\n",
      "+_bow 0.00010877508432290384\n",
      "shoot_bow 0.00010863791088937155\n",
      "hilari_bow 0.00010861772685670543\n",
      "gave_bow 0.00010849324219095489\n",
      "hill_bow 0.00010839196855860658\n",
      "beat_bow 0.00010749306603252395\n",
      "remind_bow 0.00010737227846869655\n",
      "lone_bow 0.00010723356734186414\n",
      "da_bow 0.00010632251972367069\n",
      "nite_bow 0.00010625949868407865\n",
      "fit_bow 0.00010614599599860063\n",
      "sim_bow 0.00010607122185834823\n",
      "bike_bow 0.0001059591934645933\n",
      "voic_bow 0.00010542427230181525\n",
      "green_bow 0.00010492408382030194\n",
      "past_bow 0.00010470120313614227\n",
      "si_bow 0.00010462432509403453\n",
      "fact_bow 0.0001045870237993101\n",
      "ahhh_bow 0.00010456582169985459\n",
      "scare_bow 0.00010450868540753613\n",
      "terribl_bow 0.0001043250314415453\n",
      "bday_bow 0.00010427674223215672\n",
      "becom_bow 0.00010411798005902728\n",
      "count_bow 0.00010407839556354065\n",
      "youtub_bow 0.0001040655848393248\n",
      "joke_bow 0.00010381285119447671\n",
      "stress_bow 0.00010378986127841676\n",
      "bitch_bow 0.00010377057550118461\n",
      "vacat_bow 0.00010373046457215094\n",
      "shot_bow 0.00010357099337095417\n",
      "note_bow 0.0001031822152625464\n",
      "cheer_bow 0.00010295931196010293\n",
      "app_bow 0.00010295619102565087\n",
      "cream_bow 0.00010287962697710918\n",
      "lazi_bow 0.00010279057910337116\n",
      "sold_bow 0.0001025960790257281\n",
      "ruin_bow 0.00010257844564506369\n",
      "ad_bow 0.00010233658043508927\n",
      "pray_bow 0.00010205485142036207\n",
      "via_bow 0.00010202588327645179\n",
      "doctor_bow 0.00010191300601547624\n",
      "david_bow 0.00010173123466817529\n",
      "sunshin_bow 0.00010172834839383299\n",
      "proud_bow 0.00010160203834080347\n",
      "co_bow 0.00010151343681786426\n",
      "gut_bow 0.00010144028875313019\n",
      "power_bow 0.00010140187551572989\n",
      "chillin_bow 0.00010092807315229921\n",
      "e_bow 0.00010088813381701999\n",
      "grow_bow 0.00010030690164006798\n",
      "mile_bow 9.997416639000982e-05\n",
      "myspac_bow 9.995956128484778e-05\n",
      "sent_bow 9.980735642605584e-05\n",
      "swim_bow 9.964804824263587e-05\n",
      "smell_bow 9.925378855398659e-05\n",
      "sort_bow 9.921394871026326e-05\n",
      "upload_bow 9.918397841405837e-05\n",
      "slow_bow 9.901455754337173e-05\n",
      "pull_bow 9.887706137020195e-05\n",
      "exactli_bow 9.876075212990027e-05\n",
      "ju_bow 9.79579263041923e-05\n",
      "garden_bow 9.773847029429235e-05\n",
      "lay_bow 9.772558358289589e-05\n",
      "share_bow 9.760568819731665e-05\n",
      "freak_bow 9.759093531983733e-05\n",
      "c_bow 9.755402663786426e-05\n",
      "cover_bow 9.728989039037194e-05\n",
      "bodi_bow 9.725248760144727e-05\n",
      "absolut_bow 9.667289279917313e-05\n",
      "uk_bow 9.666256885677568e-05\n",
      "paper_bow 9.613240987553704e-05\n",
      "bu_bow 9.574139570334703e-05\n",
      "whatev_bow 9.572635103592163e-05\n",
      "special_bow 9.56841292419858e-05\n",
      "tom_bow 9.528978277942291e-05\n",
      "state_bow 9.475783865126463e-05\n",
      "fell_bow 9.450005202514001e-05\n",
      "em_bow 9.442313789159525e-05\n",
      "club_bow 9.437261778459548e-05\n",
      "websit_bow 9.427902346692061e-05\n",
      "current_bow 9.403241531767153e-05\n",
      "radio_bow 9.399532504816358e-05\n",
      "allow_bow 9.391802796775352e-05\n",
      "ball_bow 9.390306495475945e-05\n",
      "burn_bow 9.38622314466246e-05\n",
      "promis_bow 9.358588098573898e-05\n",
      "hangov_bow 9.350872934848155e-05\n",
      "small_bow 9.344701748143884e-05\n",
      "jona_bow 9.334158758887086e-05\n",
      "tan_bow 9.32860103142297e-05\n",
      "gettin_bow 9.327875231318328e-05\n",
      "babe_bow 9.319664972029394e-05\n",
      "version_bow 9.314985730102944e-05\n",
      "search_bow 9.293289594035382e-05\n",
      "roll_bow 9.28626856349344e-05\n",
      "easi_bow 9.278943991020337e-05\n",
      "cloth_bow 9.255128922768839e-05\n",
      "vip_bow 9.22548093993022e-05\n",
      "met_bow 9.181532819260263e-05\n",
      "appl_bow 9.169760163716419e-05\n",
      "present_bow 9.16047835301094e-05\n",
      "lame_bow 9.15992557912042e-05\n",
      "juli_bow 9.158987449225756e-05\n",
      "earlier_bow 9.158459426266105e-05\n",
      "case_bow 9.140606941327569e-05\n",
      "round_bow 9.133921762877638e-05\n",
      "box_bow 9.119650430970683e-05\n",
      "fli_bow 9.111095961674006e-05\n",
      "coz_bow 9.063916828109895e-05\n",
      "warm_bow 9.053548434812197e-05\n",
      "comment_bow 9.049143008512387e-05\n",
      "hahah_bow 9.013388403585046e-05\n",
      "boyfriend_bow 9.011816925426377e-05\n",
      "wtf_bow 8.984910876991622e-05\n",
      "shirt_bow 8.984472301849176e-05\n",
      "event_bow 8.97699311988083e-05\n",
      "june_bow 8.957399031443445e-05\n",
      "behind_bow 8.95485778840537e-05\n",
      "film_bow 8.947436716114427e-05\n",
      "sell_bow 8.937467896515068e-05\n",
      "huge_bow 8.936394509517672e-05\n",
      "award_bow 8.91445358495876e-05\n",
      "alot_bow 8.911871034240924e-05\n",
      "expens_bow 8.870248282585373e-05\n",
      "wast_bow 8.86792255906609e-05\n",
      "white_bow 8.844280131719004e-05\n",
      "issu_bow 8.835982349655454e-05\n",
      "bag_bow 8.83188343264067e-05\n",
      "cross_bow 8.828950915464857e-05\n",
      "woo_bow 8.818648720055166e-05\n",
      "finger_bow 8.813805425613435e-05\n",
      "safe_bow 8.805312675442733e-05\n",
      "tast_bow 8.7983638076821e-05\n",
      "idk_bow 8.795639990382865e-05\n",
      "bug_bow 8.77898249486448e-05\n",
      "prepar_bow 8.771468601087222e-05\n",
      "near_bow 8.77026755189207e-05\n",
      "exhaust_bow 8.769685519376156e-05\n",
      "nah_bow 8.768844785064554e-05\n",
      "throat_bow 8.768589346240027e-05\n",
      "bbq_bow 8.70757486229389e-05\n",
      "light_bow 8.695326050592885e-05\n",
      "perform_bow 8.668879877402961e-05\n",
      "feet_bow 8.64754734032157e-05\n",
      "drunk_bow 8.621273909867759e-05\n",
      "normal_bow 8.615236033250545e-05\n",
      "camera_bow 8.606889961407444e-05\n",
      "magic_bow 8.60522184140754e-05\n",
      "random_bow 8.603834849325624e-05\n",
      "ohh_bow 8.575276842520292e-05\n",
      "dm_bow 8.565356567068144e-05\n",
      "speak_bow 8.559847798575013e-05\n",
      "slept_bow 8.54007830904968e-05\n",
      "bum_bow 8.538042764015536e-05\n",
      "cousin_bow 8.530576698036305e-05\n",
      "invit_bow 8.454335054611611e-05\n",
      "wine_bow 8.454154136689516e-05\n",
      "peac_bow 8.444765379271648e-05\n",
      "block_bow 8.444132357330252e-05\n",
      "wash_bow 8.437218675051108e-05\n",
      "product_bow 8.39636425557855e-05\n",
      "fight_bow 8.358266518254788e-05\n",
      "mail_bow 8.274244254649466e-05\n",
      "hubbi_bow 8.270919291920139e-05\n",
      "releas_bow 8.239103039754148e-05\n",
      "result_bow 8.231670100773853e-05\n",
      "googl_bow 8.2284147649947e-05\n",
      "surpris_bow 8.22794390579723e-05\n",
      "memori_bow 8.216739488102357e-05\n",
      "although_bow 8.212120543010496e-05\n",
      "lesson_bow 8.208968672383334e-05\n",
      "leg_bow 8.168270643265571e-05\n",
      "son_bow 8.156701955896839e-05\n",
      "mum_bow 8.151090884157579e-05\n",
      "ear_bow 8.117750004874738e-05\n",
      "troubl_bow 8.116919102310903e-05\n",
      "cup_bow 8.10846170615121e-05\n",
      "bar_bow 8.105168319031424e-05\n",
      "angel_bow 8.096728316644362e-05\n",
      "deal_bow 8.096557959999529e-05\n",
      "laker_bow 8.084375814588728e-05\n",
      "along_bow 8.07470590032556e-05\n",
      "pizza_bow 8.048901702020803e-05\n",
      "compani_bow 8.028462406490946e-05\n",
      "chicken_bow 7.972994314477069e-05\n",
      "kiss_bow 7.937497168311176e-05\n",
      "flu_bow 7.928807523170959e-05\n",
      "sleepi_bow 7.926094655040752e-05\n",
      "felt_bow 7.907665030885269e-05\n",
      "road_bow 7.902285558907653e-05\n",
      "caught_bow 7.883742222258863e-05\n",
      "design_bow 7.879798274227253e-05\n",
      "glass_bow 7.854113441703836e-05\n",
      "confus_bow 7.853770267983603e-05\n",
      "yup_bow 7.806361212190552e-05\n",
      "dvd_bow 7.765794728105751e-05\n",
      "dure_bow 7.760773965699195e-05\n",
      "celebr_bow 7.752487184684572e-05\n",
      "especi_bow 7.734486394568212e-05\n",
      "dark_bow 7.710957351938637e-05\n",
      "lie_bow 7.696062733704003e-05\n",
      "quotth_bow 7.676249100002424e-05\n",
      "instal_bow 7.625018179724802e-05\n",
      "mention_bow 7.618540709452552e-05\n",
      "everybodi_bow 7.610001580268008e-05\n",
      "low_bow 7.607405029628714e-05\n",
      "vega_bow 7.590558881847376e-05\n",
      "itun_bow 7.589769040027265e-05\n",
      "type_bow 7.56650845829338e-05\n",
      "pop_bow 7.561436217249729e-05\n",
      "luv_bow 7.55817253556263e-05\n",
      "stick_bow 7.552285734881344e-05\n",
      "plane_bow 7.539702382356469e-05\n",
      "yo_bow 7.529158963668096e-05\n",
      "tummi_bow 7.52228935224101e-05\n",
      "addict_bow 7.503621514796363e-05\n",
      "interview_bow 7.493832745676103e-05\n",
      "deserv_bow 7.489807305312568e-05\n",
      "countri_bow 7.489678210626382e-05\n",
      "tweetdeck_bow 7.479541680234892e-05\n",
      "doubt_bow 7.471636623400358e-05\n",
      "blackberri_bow 7.470239397875255e-05\n",
      "bro_bow 7.469266768356148e-05\n",
      "touch_bow 7.455092865188991e-05\n",
      "atam_bow 7.448140983916224e-05\n",
      "bc_bow 7.430120531213953e-05\n",
      "wednesday_bow 7.411870089209391e-05\n",
      "traffic_bow 7.409580387607119e-05\n",
      "arriv_bow 7.392963512098205e-05\n",
      "crash_bow 7.383863486040416e-05\n",
      "suggest_bow 7.376303401532324e-05\n",
      "daughter_bow 7.359059763141138e-05\n",
      "vid_bow 7.354067112218505e-05\n",
      "plu_bow 7.349091580049101e-05\n",
      "front_bow 7.323462546008566e-05\n",
      "watchin_bow 7.318111744439364e-05\n",
      "apart_bow 7.307347109321348e-05\n",
      "wit_bow 7.30481058625798e-05\n",
      "door_bow 7.29156932788154e-05\n",
      "practic_bow 7.282102006068293e-05\n",
      "fish_bow 7.280287404246115e-05\n",
      "gosh_bow 7.235580151338494e-05\n",
      "wat_bow 7.222820344736311e-05\n",
      "wife_bow 7.219582285372062e-05\n",
      "copi_bow 7.215343648612787e-05\n",
      "cd_bow 7.208378261574416e-05\n",
      "paint_bow 7.16917606591755e-05\n",
      "connect_bow 7.166729678713293e-05\n",
      "storm_bow 7.129459338451534e-05\n",
      "nobodi_bow 7.127208358607096e-05\n",
      "wors_bow 7.10986638903073e-05\n",
      "buddi_bow 7.096292885438287e-05\n",
      "expect_bow 7.091759773810894e-05\n",
      "mate_bow 7.091177029540723e-05\n",
      "prob_bow 7.090074124911184e-05\n",
      "guitar_bow 7.028135086044172e-05\n",
      "nail_bow 7.020602362464288e-05\n",
      "piss_bow 7.019190951636246e-05\n",
      "scari_bow 7.017636578646394e-05\n",
      "begin_bow 6.973361751258918e-05\n",
      "gorgeou_bow 6.969259656453462e-05\n",
      "bank_bow 6.963559845110794e-05\n",
      "stand_bow 6.945656084567667e-05\n",
      "somewher_bow 6.922062335344294e-05\n",
      "ran_bow 6.887514162707658e-05\n",
      "demi_bow 6.886721995004093e-05\n",
      "yummi_bow 6.883882079182446e-05\n",
      "gay_bow 6.86332027525781e-05\n",
      "hr_bow 6.830371222253073e-05\n",
      "tear_bow 6.82600281925344e-05\n",
      "delet_bow 6.801200650418556e-05\n",
      "fri_bow 6.790611712622515e-05\n",
      "miley_bow 6.764341160599011e-05\n",
      "web_bow 6.67856792769726e-05\n",
      "fill_bow 6.669233364673232e-05\n",
      "inde_bow 6.65205093136436e-05\n",
      "breath_bow 6.615156343470548e-05\n",
      "sexi_bow 6.598211860767986e-05\n",
      "l_bow 6.583366681970051e-05\n",
      "pm_bow 6.557116442760384e-05\n",
      "theth_bow 6.544784092083822e-05\n",
      "taken_bow 6.535369264268713e-05\n",
      "death_bow 6.515023360389302e-05\n",
      "heat_bow 6.511093853714167e-05\n",
      "quot_bow 6.500019932063836e-05\n",
      "ahead_bow 6.496012099286882e-05\n",
      "throw_bow 6.489835241276511e-05\n",
      "arm_bow 6.485286324579112e-05\n",
      "art_bow 6.470818793585647e-05\n",
      "fav_bow 6.423116833824809e-05\n",
      "channel_bow 6.398989104006294e-05\n",
      "mtv_bow 6.334307630835988e-05\n",
      "track_bow 6.333371128868322e-05\n",
      "yr_bow 6.326203169798014e-05\n",
      "experi_bow 6.268394790400442e-05\n",
      "honey_bow 6.25089310313252e-05\n",
      "servic_bow 6.245562138897184e-05\n",
      "cooki_bow 6.228584974596353e-05\n",
      "taylor_bow 6.159218466494832e-05\n",
      "xoxo_bow 6.151531924614044e-05\n",
      "travel_bow 6.140997696754393e-05\n",
      "ach_bow 6.135123355352968e-05\n",
      "doubl_bow 6.119833214756472e-05\n",
      "twilight_bow 6.104567034898801e-05\n",
      "epic_bow 6.061000640147747e-05\n",
      "profil_bow 6.040566500739381e-05\n",
      "seriou_bow 5.986620517298633e-05\n",
      "act_bow 5.9749197889075366e-05\n",
      "hasnt_bow 5.9722594559873606e-05\n",
      "dr_bow 5.968872062142054e-05\n",
      "appreci_bow 5.964777873251861e-05\n",
      "group_bow 5.878545101435941e-05\n",
      "sale_bow 5.875231335752854e-05\n",
      "shouldnt_bow 5.864536278216262e-05\n",
      "k_bow 5.836393422325446e-05\n",
      "`_bow 5.822124016277495e-05\n",
      "trek_bow 5.8212002260093294e-05\n",
      "bill_bow 5.807337906081692e-05\n",
      "thx_bow 5.7883471738806494e-05\n",
      "nick_bow 5.7848634192322754e-05\n",
      "pink_bow 5.783438312454074e-05\n",
      "dunno_bow 5.7765290611895e-05\n",
      "view_bow 5.764810643581158e-05\n",
      "chri_bow 5.730734188690402e-05\n",
      "def_bow 5.725523751222781e-05\n",
      "fantast_bow 5.722539464723061e-05\n",
      "puppi_bow 5.70308875942326e-05\n",
      "holi_bow 5.5893632760912245e-05\n",
      "knee_bow 5.555686457060346e-05\n",
      "stomach_bow 5.5054191280150585e-05\n",
      "yum_bow 5.481927798485186e-05\n",
      "ouch_bow 5.406009541461102e-05\n",
      "edit_bow 5.39985687530128e-05\n",
      "brain_bow 5.3937353757115014e-05\n",
      "followfriday_bow 5.38361823213943e-05\n",
      "color_bow 5.356220863671308e-05\n",
      "ador_bow 5.308697314044154e-05\n",
      "screen_bow 5.2233495868161486e-05\n",
      "st_bow 5.117816614355843e-05\n",
      "singl_bow 5.112201207936751e-05\n",
      "dang_bow 5.0930212127372236e-05\n",
      "mommi_bow 4.910438543839221e-05\n",
      "recommend_bow 4.9027129988596035e-05\n",
      "nose_bow 4.8707319952539106e-05\n",
      "bloodi_bow 4.8026362910908075e-05\n",
      "choic_bow 4.77460285703266e-05\n",
      "street_bow 4.760504335856036e-05\n",
      "ff_bow 3.940586280345894e-05\n",
      "«_bow 3.475415895097175e-05\n",
      "»_bow 2.4711553086363832e-05\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "for idx, fi in enumerate(classifier.feature_importances_):\n",
    "    features[data_model.columns[1+idx]] = fi\n",
    "\n",
    "important = []\n",
    "for f in sorted(features,key=features.get,reverse=True):\n",
    "    important.append((f,features[f]))\n",
    "    print(f + \" \" + str(features[f]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================\n",
      "Testing RandomForestClassifier\n",
      "Learing time 61.461753606796265s\n",
      "Predicting time 1.4276564121246338s\n",
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.75139878 0.74797785]\n",
      "Precision[0.74633344 0.75315985]\n",
      "Recall   [0.75653333 0.74286667]\n",
      "Accuracy 0.7497\n",
      "===============================================\n",
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 771.9572877883911s\n",
      "Accuracy: [0.74504    0.744      0.7532     0.7532     0.75384    0.75064\n",
      " 0.748      0.74653972]\n",
      "Average accuracy: 0.7493074653972318\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1,random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the average accuracy from crossvalidation is almost 58%, and the results from the crossvalidation runs are more stable - they never drop below 51%. It might seem that the algorithm will perform pretty well on the testing set. There's a one problem with that - recall for the 7:3 split shows, that only about 3% of the negative tweets from the whole set were recognized properly - it seems like the algorithm is good with recognition of postive vs. neutral cases, but it does really poor job when it comes to recognize negative sentiment. Let's try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing test dataset with \"gene editing\" related Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(path.join(dataset_dir, \"gene_testdataset.csv\"), names=[\"id\", \"emotion\", \"text\"])\n",
    "test_dataset = test_dataset[test_dataset != \"neutral\"]\n",
    "test_dataset.groupby(\"emotion\").count()\n",
    "test_dataset.to_csv(path.join(dataset_dir, \"filtered_gene_testdataset.csv\"), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvasov/anaconda3/envs/sen_anal/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>number_of_uppercase</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_urls</th>\n",
       "      <th>number_of_positive_emo</th>\n",
       "      <th>...</th>\n",
       "      <th>bill_bow</th>\n",
       "      <th>expens_bow</th>\n",
       "      <th>bank_bow</th>\n",
       "      <th>prob_bow</th>\n",
       "      <th>quotth_bow</th>\n",
       "      <th>honey_bow</th>\n",
       "      <th>choic_bow</th>\n",
       "      <th>group_bow</th>\n",
       "      <th>trek_bow</th>\n",
       "      <th>chri_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  number_of_uppercase  number_of_exclamation  number_of_question  \\\n",
       "0  positive                   14                      1                   0   \n",
       "1  positive                   13                      0                   0   \n",
       "2  positive                    8                      0                   0   \n",
       "3  negative                    6                      0                   0   \n",
       "4  negative                    7                      0                   0   \n",
       "\n",
       "   number_of_ellipsis  number_of_hashtags  number_of_mentions  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   1                   1   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   number_of_quotes  number_of_urls  number_of_positive_emo    ...     \\\n",
       "0                 0               0                       0    ...      \n",
       "1                 0               0                       0    ...      \n",
       "2                 0               0                       0    ...      \n",
       "3                 0               0                       0    ...      \n",
       "4                 0               0                       0    ...      \n",
       "\n",
       "   bill_bow  expens_bow  bank_bow  prob_bow  quotth_bow  honey_bow  choic_bow  \\\n",
       "0         0           0         0         0           0          0          0   \n",
       "1         0           0         0         0           0          0          0   \n",
       "2         0           0         0         0           0          0          0   \n",
       "3         0           0         0         0           0          0          0   \n",
       "4         0           0         0         0           0          0          0   \n",
       "\n",
       "   group_bow  trek_bow  chri_bow  \n",
       "0          0         0         0  \n",
       "1          1         0         0  \n",
       "2          0         0         0  \n",
       "3          0         0         0  \n",
       "4          0         0         0  \n",
       "\n",
       "[5 rows x 1027 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = TwitterData()\n",
    "test_data.initialize(path.join(dataset_dir, \"filtered_gene_testdataset.csv\"))\n",
    "test_data.build_features()\n",
    "test_data.cleanup(TwitterCleanuper())\n",
    "test_data.tokenize()\n",
    "test_data.stem()\n",
    "test_data.build_wordlist()\n",
    "test_data.build_final_model(word2vec)\n",
    "test_data.data_model.drop(\"original_id\",axis=1,inplace=True)\n",
    "\n",
    "test_data.data_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Results ===================\n",
      "            Negative     Neutral     Positive\n",
      "F1       [0.61538462 0.75      ]\n",
      "Precision[0.58536585 0.77586207]\n",
      "Recall   [0.64864865 0.72580645]\n",
      "Accuracy 0.696969696969697\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "list_of_labels = sorted(list(set(y_train)))\n",
    "\n",
    "X_test = test_data.data_model.iloc[:,1:]\n",
    "y_test = test_data.data_model.iloc[:,0]\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "log(\"=================== Results ===================\")\n",
    "log(\"            Negative     Neutral     Positive\")\n",
    "log(\"F1       \" + str(f1))\n",
    "log(\"Precision\" + str(precision))\n",
    "log(\"Recall   \" + str(recall))\n",
    "log(\"Accuracy \" + str(accuracy))\n",
    "log(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "Let's take a look at the final model feature importance.\n",
    "\n",
    "*Output list is really long, so the print line is commented in the code block*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for idx, fi in enumerate(xgboost.feature_importances_):\n",
    "    features[test_model.columns[1+idx]] = fi\n",
    "\n",
    "important = []\n",
    "for f in sorted(features,key=features.get,reverse=True):\n",
    "    important.append((f,features[f]))\n",
    "    # print(f + \" \" + str(features[f]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not surprise that the word2vec and related good/bad/information similarity features were the most important, because the classification performance was greatly improved after switching to this representation.\n",
    "\n",
    "What is interesting is that a lot of custom-crafted features (number\\_of\\_\\*) were also highly important, beating a lot of features which came from bag-of-words representation.\n",
    "Here's the chart for the easiness of reading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"2708733e-6e01-452e-ae65-53294bf9311b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2708733e-6e01-452e-ae65-53294bf9311b\", [{\"orientation\": \"h\", \"type\": \"bar\", \"y\": [\"didnt_bow\", \"parenthood_bow\", \"not_bow\", \"thi_bow\", \"number_of_negative_emo\", \"want_bow\", \"plan_bow\", \"see_bow\", \"im_bow\", \"number_of_ellipsis\", \"number_of_quotes\", \"fuck_bow\", \"best_bow\", \"number_of_hashtags\", \"palin_bow\", \"number_of_mentions\", \"number_of_urls\", \"number_of_question\", \"number_of_uppercase\", \"number_of_exclamation\"], \"x\": [0.00011539919796632603, 0.0001318847935181111, 0.0001483703963458538, 0.0001483703963458538, 0.0001648559991735965, 0.0001648559991735965, 0.0001978272048290819, 0.00023079839593265206, 0.0002637695870362222, 0.0002802551898639649, 0.0002802551898639649, 0.0003791688068304211, 0.0003791688068304211, 0.000412139983382076, 0.0005110536003485322, 0.0006264527910389006, 0.0006759095704182982, 0.0013847904046997428, 0.0015331608010455966, 0.0032311775721609592]}], {\"title\": \"Most important features in the final model\", \"margin\": {\"l\": 200, \"pad\": 3}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_show = list(filter(lambda f: not f[0].startswith(\"word2vec\") and not f[0].endswith(\"_similarity\"),important))[:20]\n",
    "to_show.reverse()\n",
    "features_importance = [\n",
    "    graph_objs.Bar(\n",
    "        x=[f[1] for f in to_show],\n",
    "        y=[f[0] for f in to_show],\n",
    "        orientation=\"h\"\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":features_importance, \"layout\":graph_objs.Layout(title=\"Most important features in the final model\",\n",
    "      margin=graph_objs.Margin(\n",
    "           l=200,\n",
    "        pad=3\n",
    "    ),)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost considered custom features as important, so it also confirmed that some non-word features of a text can also be used to predict the sentiment. Most of them were even more important than the actual presence of some emotion-expressing words in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Experiment showed that prediction of text sentiment is a non-trivial task for machine learning. A lot of preprocessing is required just to be able to run any algorithm and see - usually not great - results. \n",
    "Main problem for sentiment analysis is to craft the machine representation of the text. Simple bag-of-words was definitely not enough to obtain satisfying results, thus a lot of additional features were created basing on common sense (number of emoticons, exclamation marks etc.). Word2vec representation significantly raised the predictions quality.\n",
    "I think that a slight improvement in classification accuracy for the given training dataset could be developed, but since it contained highly skewed data (small number of negative cases), the difference will be probably in the order of a few percent.\n",
    "The thing that could possibly improve classification results will be to add a lot of additional examples (increase training dataset), because given 5971 examples obviously do not contain every combination of words usage, moreover - a lot of emotion-expressing words surely are missing."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
